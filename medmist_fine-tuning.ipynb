{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from datasets import Dataset as HFDataset, DatasetDict\n",
    "from medmnist import BreastMNIST, PneumoniaMNIST\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn.utils import resample\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Parsing\n",
    "The fist step was to convert the MedMNIST dataset into a format suitable for Hugging Face model, either the datsets suffering by strong unbalancing so we implemented a balanced version for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedMNISTtoHF(Dataset):\n",
    "    def __init__(self, medmnist_dataset):\n",
    "        \"\"\"\n",
    "        Convert MedMNIST dataset to a format compatible with HuggingFace models\n",
    "        Args:\n",
    "            medmnist_dataset: The original MedMNIST dataset\n",
    "            transform: Optional transforms to be applied to the images\n",
    "        \"\"\"\n",
    "        self.dataset = medmnist_dataset\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "\n",
    "        return {\n",
    "            \"image\": img,\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def convert_medmnist_to_hf(medmnist_dataset):\n",
    "    \"\"\"\n",
    "    Convert a MedMNIST dataset to a HuggingFace dataset\n",
    "    Args:\n",
    "        medmnist_dataset: The original MedMNIST dataset\n",
    "    Returns:\n",
    "        The HuggingFace dataset\n",
    "    \"\"\"\n",
    "    # Create wrapper dataset\n",
    "    wrapper_dataset = MedMNISTtoHF(medmnist_dataset)\n",
    "    \n",
    "    # Convert to HF format\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(len(wrapper_dataset)):\n",
    "        sample = wrapper_dataset[i]\n",
    "        images.append(sample[\"image\"])\n",
    "        labels.append(sample[\"label\"].item())\n",
    "    \n",
    "    # Create HF dataset\n",
    "    hf_dataset = HFDataset.from_dict({\n",
    "        \"image\": images,\n",
    "        \"label\": labels\n",
    "    })\n",
    "    \n",
    "    return hf_dataset\n",
    "\n",
    "def dataset_balancing(dataset, alpha):\n",
    "    \"\"\"\t\n",
    "    Balance the dataset by oversampling the minority class\n",
    "    Args:\n",
    "        dataset: The original dataset\n",
    "        alpha: The oversampling factor\n",
    "    Returns:\n",
    "        The balanced dataset\n",
    "    \"\"\"\n",
    "\n",
    "    mj_class = Counter(dataset['label']).most_common(1)[0][0] # majority class\n",
    "    mn_class = abs(mj_class-1) # minority class\n",
    "    data = dataset['image'] # images extracted from the dataset\n",
    "\n",
    "    mask = [lb== mj_class for lb in dataset['label']] # mask to separate majority and minority class\n",
    "    X_majority = [img for img,flag in zip(data, mask) if flag] # majority class images\n",
    "    X_minority = [img for img,flag in zip(data, mask) if not flag] # minority class images\n",
    "    new_len_majority = len(X_minority) + int(alpha*len(X_minority)) # new length of the majority class\n",
    "    X_majority_resampled = resample(X_majority, \n",
    "                                    replace=False,  # No replacement\n",
    "                                    n_samples=new_len_majority,  # Match minority class size\n",
    "                                    random_state=42)\n",
    "    X_resampled = X_majority_resampled + X_minority # resampled dataset\n",
    "    y_resampled = [mj_class]*new_len_majority + [mn_class]*len(X_minority) # resampled labels\n",
    "    random.seed(42)\n",
    "    random.shuffle(X_resampled) # shuffle the dataset\n",
    "    random.seed(42)\n",
    "    random.shuffle(y_resampled) # shuffle the labels\n",
    "\n",
    "    dict_blanced_dataset = {\n",
    "        \"image\": X_resampled,\n",
    "        \"label\": y_resampled\n",
    "    } # dictionary of the balanced dataset\n",
    "    balanced_dataset = datasets.Dataset.from_dict(dict_blanced_dataset) # convert the dictionary to a HuggingFace dataset\n",
    "    return balanced_dataset\n",
    "\n",
    "def load_dataset_medmnist(dataset_name, size, balancing):\n",
    "    \"\"\"\n",
    "    Load a MedMNIST dataset and convert it to a HuggingFace dataset\n",
    "    Args:\n",
    "        dataset_name: The name of the MedMNIST dataset\n",
    "        size: The size of the images\n",
    "        balancing: Whether to balance the dataset\n",
    "    Returns:\n",
    "        The HuggingFace dataset\n",
    "    \"\"\"\n",
    "    # Load MedMNIST dataset\n",
    "    train_dataset = dataset_name(split='train', download=True, size=size) \n",
    "    val_dataset = dataset_name(split='val', download=True, size=size) \n",
    "    test_dataset = dataset_name(split='test', download=True, size=size)\n",
    "\n",
    "    # Convert to HuggingFace dataset\n",
    "    hf_train_dataset = convert_medmnist_to_hf(train_dataset)\n",
    "    hf_val_dataset = convert_medmnist_to_hf(val_dataset)\n",
    "    hf_test_dataset = convert_medmnist_to_hf(test_dataset)\n",
    "\n",
    "    # Balancing\n",
    "    if balancing: \n",
    "        hf_train_dataset_balanced = dataset_balancing(hf_train_dataset, 0.5) # balance the training dataset\n",
    "        dataset = DatasetDict({\"train\": hf_train_dataset_balanced, \"validation\": hf_val_dataset, \"test\": hf_test_dataset}) \n",
    "        return dataset\n",
    "    else:\n",
    "        dataset = DatasetDict({\"train\": hf_train_dataset, \"validation\": hf_val_dataset, \"test\": hf_test_dataset})\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist_224.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist_224.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist_224.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist_224.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist_224.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist_224.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\pneumoniamnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\pneumoniamnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\pneumoniamnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\pneumoniamnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\pneumoniamnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\pneumoniamnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\pneumoniamnist_224.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\pneumoniamnist_224.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\pneumoniamnist_224.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\pneumoniamnist_224.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\pneumoniamnist_224.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\pneumoniamnist_224.npz\n"
     ]
    }
   ],
   "source": [
    "breast_dataset_28 = load_dataset_medmnist(BreastMNIST, 28, False)\n",
    "breast_dataset_balanced_28 = load_dataset_medmnist(BreastMNIST, 28, True)\n",
    "breast_dataset_224 = load_dataset_medmnist(BreastMNIST, 224, False)\n",
    "breast_dataset_balanced_224 = load_dataset_medmnist(BreastMNIST, 224, True)\n",
    "\n",
    "pneumonia_dataset_28 = load_dataset_medmnist(PneumoniaMNIST, 28, False)\n",
    "pneumonia_dataset_balanced_28 = load_dataset_medmnist(PneumoniaMNIST, 28, True)\n",
    "pneumonia_dataset_224 = load_dataset_medmnist(PneumoniaMNIST, 224, False)\n",
    "pneumonia_dataset_balanced_224 = load_dataset_medmnist(PneumoniaMNIST, 224, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freezer Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beit_freezer(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(\"classifier\") \\\n",
    "            and not name.startswith(\"beit.pooler\")\\\n",
    "            and not name.startswith(\"beit.encoder.layer.23\")\\\n",
    "            and not name.startswith(\"beit.encoder.layer.22\")\\\n",
    "            and not name.startswith(\"beit.encoder.layer.21\")\\\n",
    "            and not name.startswith(\"beit.encoder.layer.20\")\\\n",
    "            and not name.startswith(\"beit.encoder.layer.19\"):\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_reproducibility(seed=42):\n",
    "    # Set seeds for reproducibility\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T_and_T(dataset, model_name, output_dir, batch_size, weight_decay, Training, Testing, freezer):\n",
    "    # Set Seed\n",
    "    set_reproducibility()\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Check if GPU is available\n",
    "    \n",
    "    # Load the BEiT-large model and image processor\n",
    "    model = AutoModelForImageClassification.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True).to(device) # Load the model\n",
    "    processor = AutoImageProcessor.from_pretrained(model_name) # Load the image processor\n",
    "\n",
    "    # Freeze some layers of the model\n",
    "    freezer(model) \n",
    "\n",
    "    # Verify which layers are trainable\n",
    "    trainable_params = [name for name, param in model.named_parameters() if param.requires_grad]\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "\n",
    "    # Define preprocessing function\n",
    "    def preprocess_images(examples):\n",
    "        images = [processor(image.convert(\"RGB\"), return_tensors=\"pt\") for image in examples[\"image\"]] # Convert images to RGB format\n",
    "        pixel_values = torch.stack([image[\"pixel_values\"].squeeze() for image in images]) # Stack the pixel values\n",
    "        labels = torch.tensor(examples[\"label\"]) # Get the labels\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": labels} # Return the pixel values and labels\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    train_dataset = dataset[\"train\"].with_transform(preprocess_images)\n",
    "    validation_dataset = dataset[\"validation\"].with_transform(preprocess_images)\n",
    "    test_dataset = dataset[\"test\"].with_transform(preprocess_images)\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=3e-4,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=weight_decay, # Regularization\n",
    "        logging_dir=\"./logs\", \n",
    "        logging_steps=10,\n",
    "        save_total_limit=1,\n",
    "        remove_unused_columns=False,\n",
    "        push_to_hub=False,\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    # Define Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset,\n",
    "        tokenizer=processor,\n",
    "    )\n",
    "    if Training:\n",
    "        trainer.train()\n",
    "        model.save_pretrained(output_dir)\n",
    "        processor.save_pretrained(output_dir)\n",
    "\n",
    "    if Testing:\n",
    "        predictions = trainer.predict(test_dataset)\n",
    "        pred_labels = predictions.predictions.argmax(axis=1)\n",
    "        true_labels = predictions.label_ids\n",
    "\n",
    "        # Calculate the metrics\n",
    "        accuracy = accuracy_score(true_labels, pred_labels)\n",
    "        auc = roc_auc_score(true_labels, pred_labels)\n",
    "\n",
    "        # Display the metrics\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"AUC: {auc:.4f}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-large-patch16-224-pt22k and are newly initialized: ['beit.pooler.layernorm.bias', 'beit.pooler.layernorm.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['beit.encoder.layer.19.lambda_1', 'beit.encoder.layer.19.lambda_2', 'beit.encoder.layer.19.attention.attention.query.weight', 'beit.encoder.layer.19.attention.attention.query.bias', 'beit.encoder.layer.19.attention.attention.key.weight', 'beit.encoder.layer.19.attention.attention.value.weight', 'beit.encoder.layer.19.attention.attention.value.bias', 'beit.encoder.layer.19.attention.output.dense.weight', 'beit.encoder.layer.19.attention.output.dense.bias', 'beit.encoder.layer.19.intermediate.dense.weight', 'beit.encoder.layer.19.intermediate.dense.bias', 'beit.encoder.layer.19.output.dense.weight', 'beit.encoder.layer.19.output.dense.bias', 'beit.encoder.layer.19.layernorm_before.weight', 'beit.encoder.layer.19.layernorm_before.bias', 'beit.encoder.layer.19.layernorm_after.weight', 'beit.encoder.layer.19.layernorm_after.bias', 'beit.encoder.layer.20.lambda_1', 'beit.encoder.layer.20.lambda_2', 'beit.encoder.layer.20.attention.attention.query.weight', 'beit.encoder.layer.20.attention.attention.query.bias', 'beit.encoder.layer.20.attention.attention.key.weight', 'beit.encoder.layer.20.attention.attention.value.weight', 'beit.encoder.layer.20.attention.attention.value.bias', 'beit.encoder.layer.20.attention.output.dense.weight', 'beit.encoder.layer.20.attention.output.dense.bias', 'beit.encoder.layer.20.intermediate.dense.weight', 'beit.encoder.layer.20.intermediate.dense.bias', 'beit.encoder.layer.20.output.dense.weight', 'beit.encoder.layer.20.output.dense.bias', 'beit.encoder.layer.20.layernorm_before.weight', 'beit.encoder.layer.20.layernorm_before.bias', 'beit.encoder.layer.20.layernorm_after.weight', 'beit.encoder.layer.20.layernorm_after.bias', 'beit.encoder.layer.21.lambda_1', 'beit.encoder.layer.21.lambda_2', 'beit.encoder.layer.21.attention.attention.query.weight', 'beit.encoder.layer.21.attention.attention.query.bias', 'beit.encoder.layer.21.attention.attention.key.weight', 'beit.encoder.layer.21.attention.attention.value.weight', 'beit.encoder.layer.21.attention.attention.value.bias', 'beit.encoder.layer.21.attention.output.dense.weight', 'beit.encoder.layer.21.attention.output.dense.bias', 'beit.encoder.layer.21.intermediate.dense.weight', 'beit.encoder.layer.21.intermediate.dense.bias', 'beit.encoder.layer.21.output.dense.weight', 'beit.encoder.layer.21.output.dense.bias', 'beit.encoder.layer.21.layernorm_before.weight', 'beit.encoder.layer.21.layernorm_before.bias', 'beit.encoder.layer.21.layernorm_after.weight', 'beit.encoder.layer.21.layernorm_after.bias', 'beit.encoder.layer.22.lambda_1', 'beit.encoder.layer.22.lambda_2', 'beit.encoder.layer.22.attention.attention.query.weight', 'beit.encoder.layer.22.attention.attention.query.bias', 'beit.encoder.layer.22.attention.attention.key.weight', 'beit.encoder.layer.22.attention.attention.value.weight', 'beit.encoder.layer.22.attention.attention.value.bias', 'beit.encoder.layer.22.attention.output.dense.weight', 'beit.encoder.layer.22.attention.output.dense.bias', 'beit.encoder.layer.22.intermediate.dense.weight', 'beit.encoder.layer.22.intermediate.dense.bias', 'beit.encoder.layer.22.output.dense.weight', 'beit.encoder.layer.22.output.dense.bias', 'beit.encoder.layer.22.layernorm_before.weight', 'beit.encoder.layer.22.layernorm_before.bias', 'beit.encoder.layer.22.layernorm_after.weight', 'beit.encoder.layer.22.layernorm_after.bias', 'beit.encoder.layer.23.lambda_1', 'beit.encoder.layer.23.lambda_2', 'beit.encoder.layer.23.attention.attention.query.weight', 'beit.encoder.layer.23.attention.attention.query.bias', 'beit.encoder.layer.23.attention.attention.key.weight', 'beit.encoder.layer.23.attention.attention.value.weight', 'beit.encoder.layer.23.attention.attention.value.bias', 'beit.encoder.layer.23.attention.output.dense.weight', 'beit.encoder.layer.23.attention.output.dense.bias', 'beit.encoder.layer.23.intermediate.dense.weight', 'beit.encoder.layer.23.intermediate.dense.bias', 'beit.encoder.layer.23.output.dense.weight', 'beit.encoder.layer.23.output.dense.bias', 'beit.encoder.layer.23.layernorm_before.weight', 'beit.encoder.layer.23.layernorm_before.bias', 'beit.encoder.layer.23.layernorm_after.weight', 'beit.encoder.layer.23.layernorm_after.bias', 'beit.pooler.layernorm.weight', 'beit.pooler.layernorm.bias', 'classifier.weight', 'classifier.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_10660\\2099681282.py:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90/90 01:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.233300</td>\n",
       "      <td>0.567551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.592600</td>\n",
       "      <td>0.543375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.471200</td>\n",
       "      <td>0.417184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.385600</td>\n",
       "      <td>0.368475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.329900</td>\n",
       "      <td>0.293840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8910\n",
      "AUC: 0.8202\n"
     ]
    }
   ],
   "source": [
    "T_and_T(breast_dataset_28, \"microsoft/beit-large-patch16-224-pt22k\", \"beit_breast_28\", 32, 0.01, True, True, beit_freezer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-large-patch16-224-pt22k and are newly initialized: ['beit.pooler.layernorm.bias', 'beit.pooler.layernorm.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['beit.encoder.layer.19.lambda_1', 'beit.encoder.layer.19.lambda_2', 'beit.encoder.layer.19.attention.attention.query.weight', 'beit.encoder.layer.19.attention.attention.query.bias', 'beit.encoder.layer.19.attention.attention.key.weight', 'beit.encoder.layer.19.attention.attention.value.weight', 'beit.encoder.layer.19.attention.attention.value.bias', 'beit.encoder.layer.19.attention.output.dense.weight', 'beit.encoder.layer.19.attention.output.dense.bias', 'beit.encoder.layer.19.intermediate.dense.weight', 'beit.encoder.layer.19.intermediate.dense.bias', 'beit.encoder.layer.19.output.dense.weight', 'beit.encoder.layer.19.output.dense.bias', 'beit.encoder.layer.19.layernorm_before.weight', 'beit.encoder.layer.19.layernorm_before.bias', 'beit.encoder.layer.19.layernorm_after.weight', 'beit.encoder.layer.19.layernorm_after.bias', 'beit.encoder.layer.20.lambda_1', 'beit.encoder.layer.20.lambda_2', 'beit.encoder.layer.20.attention.attention.query.weight', 'beit.encoder.layer.20.attention.attention.query.bias', 'beit.encoder.layer.20.attention.attention.key.weight', 'beit.encoder.layer.20.attention.attention.value.weight', 'beit.encoder.layer.20.attention.attention.value.bias', 'beit.encoder.layer.20.attention.output.dense.weight', 'beit.encoder.layer.20.attention.output.dense.bias', 'beit.encoder.layer.20.intermediate.dense.weight', 'beit.encoder.layer.20.intermediate.dense.bias', 'beit.encoder.layer.20.output.dense.weight', 'beit.encoder.layer.20.output.dense.bias', 'beit.encoder.layer.20.layernorm_before.weight', 'beit.encoder.layer.20.layernorm_before.bias', 'beit.encoder.layer.20.layernorm_after.weight', 'beit.encoder.layer.20.layernorm_after.bias', 'beit.encoder.layer.21.lambda_1', 'beit.encoder.layer.21.lambda_2', 'beit.encoder.layer.21.attention.attention.query.weight', 'beit.encoder.layer.21.attention.attention.query.bias', 'beit.encoder.layer.21.attention.attention.key.weight', 'beit.encoder.layer.21.attention.attention.value.weight', 'beit.encoder.layer.21.attention.attention.value.bias', 'beit.encoder.layer.21.attention.output.dense.weight', 'beit.encoder.layer.21.attention.output.dense.bias', 'beit.encoder.layer.21.intermediate.dense.weight', 'beit.encoder.layer.21.intermediate.dense.bias', 'beit.encoder.layer.21.output.dense.weight', 'beit.encoder.layer.21.output.dense.bias', 'beit.encoder.layer.21.layernorm_before.weight', 'beit.encoder.layer.21.layernorm_before.bias', 'beit.encoder.layer.21.layernorm_after.weight', 'beit.encoder.layer.21.layernorm_after.bias', 'beit.encoder.layer.22.lambda_1', 'beit.encoder.layer.22.lambda_2', 'beit.encoder.layer.22.attention.attention.query.weight', 'beit.encoder.layer.22.attention.attention.query.bias', 'beit.encoder.layer.22.attention.attention.key.weight', 'beit.encoder.layer.22.attention.attention.value.weight', 'beit.encoder.layer.22.attention.attention.value.bias', 'beit.encoder.layer.22.attention.output.dense.weight', 'beit.encoder.layer.22.attention.output.dense.bias', 'beit.encoder.layer.22.intermediate.dense.weight', 'beit.encoder.layer.22.intermediate.dense.bias', 'beit.encoder.layer.22.output.dense.weight', 'beit.encoder.layer.22.output.dense.bias', 'beit.encoder.layer.22.layernorm_before.weight', 'beit.encoder.layer.22.layernorm_before.bias', 'beit.encoder.layer.22.layernorm_after.weight', 'beit.encoder.layer.22.layernorm_after.bias', 'beit.encoder.layer.23.lambda_1', 'beit.encoder.layer.23.lambda_2', 'beit.encoder.layer.23.attention.attention.query.weight', 'beit.encoder.layer.23.attention.attention.query.bias', 'beit.encoder.layer.23.attention.attention.key.weight', 'beit.encoder.layer.23.attention.attention.value.weight', 'beit.encoder.layer.23.attention.attention.value.bias', 'beit.encoder.layer.23.attention.output.dense.weight', 'beit.encoder.layer.23.attention.output.dense.bias', 'beit.encoder.layer.23.intermediate.dense.weight', 'beit.encoder.layer.23.intermediate.dense.bias', 'beit.encoder.layer.23.output.dense.weight', 'beit.encoder.layer.23.output.dense.bias', 'beit.encoder.layer.23.layernorm_before.weight', 'beit.encoder.layer.23.layernorm_before.bias', 'beit.encoder.layer.23.layernorm_after.weight', 'beit.encoder.layer.23.layernorm_after.bias', 'beit.pooler.layernorm.weight', 'beit.pooler.layernorm.bias', 'classifier.weight', 'classifier.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_20840\\2099681282.py:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='115' max='115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [115/115 01:09, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.682800</td>\n",
       "      <td>0.711752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.628900</td>\n",
       "      <td>0.442710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.592400</td>\n",
       "      <td>0.359735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.451900</td>\n",
       "      <td>0.278309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.373300</td>\n",
       "      <td>0.277980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8846\n",
      "AUC: 0.8459\n"
     ]
    }
   ],
   "source": [
    "T_and_T(breast_dataset_balanced_28, \"microsoft/beit-large-patch16-224-pt22k\", \"beit_breast_balanced_28\", 16, 0, True, True, beit_freezer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-large-patch16-224-pt22k and are newly initialized: ['beit.pooler.layernorm.bias', 'beit.pooler.layernorm.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['beit.encoder.layer.19.lambda_1', 'beit.encoder.layer.19.lambda_2', 'beit.encoder.layer.19.attention.attention.query.weight', 'beit.encoder.layer.19.attention.attention.query.bias', 'beit.encoder.layer.19.attention.attention.key.weight', 'beit.encoder.layer.19.attention.attention.value.weight', 'beit.encoder.layer.19.attention.attention.value.bias', 'beit.encoder.layer.19.attention.output.dense.weight', 'beit.encoder.layer.19.attention.output.dense.bias', 'beit.encoder.layer.19.intermediate.dense.weight', 'beit.encoder.layer.19.intermediate.dense.bias', 'beit.encoder.layer.19.output.dense.weight', 'beit.encoder.layer.19.output.dense.bias', 'beit.encoder.layer.19.layernorm_before.weight', 'beit.encoder.layer.19.layernorm_before.bias', 'beit.encoder.layer.19.layernorm_after.weight', 'beit.encoder.layer.19.layernorm_after.bias', 'beit.encoder.layer.20.lambda_1', 'beit.encoder.layer.20.lambda_2', 'beit.encoder.layer.20.attention.attention.query.weight', 'beit.encoder.layer.20.attention.attention.query.bias', 'beit.encoder.layer.20.attention.attention.key.weight', 'beit.encoder.layer.20.attention.attention.value.weight', 'beit.encoder.layer.20.attention.attention.value.bias', 'beit.encoder.layer.20.attention.output.dense.weight', 'beit.encoder.layer.20.attention.output.dense.bias', 'beit.encoder.layer.20.intermediate.dense.weight', 'beit.encoder.layer.20.intermediate.dense.bias', 'beit.encoder.layer.20.output.dense.weight', 'beit.encoder.layer.20.output.dense.bias', 'beit.encoder.layer.20.layernorm_before.weight', 'beit.encoder.layer.20.layernorm_before.bias', 'beit.encoder.layer.20.layernorm_after.weight', 'beit.encoder.layer.20.layernorm_after.bias', 'beit.encoder.layer.21.lambda_1', 'beit.encoder.layer.21.lambda_2', 'beit.encoder.layer.21.attention.attention.query.weight', 'beit.encoder.layer.21.attention.attention.query.bias', 'beit.encoder.layer.21.attention.attention.key.weight', 'beit.encoder.layer.21.attention.attention.value.weight', 'beit.encoder.layer.21.attention.attention.value.bias', 'beit.encoder.layer.21.attention.output.dense.weight', 'beit.encoder.layer.21.attention.output.dense.bias', 'beit.encoder.layer.21.intermediate.dense.weight', 'beit.encoder.layer.21.intermediate.dense.bias', 'beit.encoder.layer.21.output.dense.weight', 'beit.encoder.layer.21.output.dense.bias', 'beit.encoder.layer.21.layernorm_before.weight', 'beit.encoder.layer.21.layernorm_before.bias', 'beit.encoder.layer.21.layernorm_after.weight', 'beit.encoder.layer.21.layernorm_after.bias', 'beit.encoder.layer.22.lambda_1', 'beit.encoder.layer.22.lambda_2', 'beit.encoder.layer.22.attention.attention.query.weight', 'beit.encoder.layer.22.attention.attention.query.bias', 'beit.encoder.layer.22.attention.attention.key.weight', 'beit.encoder.layer.22.attention.attention.value.weight', 'beit.encoder.layer.22.attention.attention.value.bias', 'beit.encoder.layer.22.attention.output.dense.weight', 'beit.encoder.layer.22.attention.output.dense.bias', 'beit.encoder.layer.22.intermediate.dense.weight', 'beit.encoder.layer.22.intermediate.dense.bias', 'beit.encoder.layer.22.output.dense.weight', 'beit.encoder.layer.22.output.dense.bias', 'beit.encoder.layer.22.layernorm_before.weight', 'beit.encoder.layer.22.layernorm_before.bias', 'beit.encoder.layer.22.layernorm_after.weight', 'beit.encoder.layer.22.layernorm_after.bias', 'beit.encoder.layer.23.lambda_1', 'beit.encoder.layer.23.lambda_2', 'beit.encoder.layer.23.attention.attention.query.weight', 'beit.encoder.layer.23.attention.attention.query.bias', 'beit.encoder.layer.23.attention.attention.key.weight', 'beit.encoder.layer.23.attention.attention.value.weight', 'beit.encoder.layer.23.attention.attention.value.bias', 'beit.encoder.layer.23.attention.output.dense.weight', 'beit.encoder.layer.23.attention.output.dense.bias', 'beit.encoder.layer.23.intermediate.dense.weight', 'beit.encoder.layer.23.intermediate.dense.bias', 'beit.encoder.layer.23.output.dense.weight', 'beit.encoder.layer.23.output.dense.bias', 'beit.encoder.layer.23.layernorm_before.weight', 'beit.encoder.layer.23.layernorm_before.bias', 'beit.encoder.layer.23.layernorm_after.weight', 'beit.encoder.layer.23.layernorm_after.bias', 'beit.pooler.layernorm.weight', 'beit.pooler.layernorm.bias', 'classifier.weight', 'classifier.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_20840\\2099681282.py:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [175/175 01:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.581100</td>\n",
       "      <td>0.421221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.501900</td>\n",
       "      <td>0.303188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.403900</td>\n",
       "      <td>0.276487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.122400</td>\n",
       "      <td>0.250388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.155500</td>\n",
       "      <td>0.296184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8782\n",
      "AUC: 0.7964\n"
     ]
    }
   ],
   "source": [
    "T_and_T(breast_dataset_224, \"microsoft/beit-large-patch16-224-pt22k\", \"beit_breast_224\", 16, 0.1, True, True, beit_freezer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-large-patch16-224-pt22k and are newly initialized: ['beit.pooler.layernorm.bias', 'beit.pooler.layernorm.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['beit.encoder.layer.19.lambda_1', 'beit.encoder.layer.19.lambda_2', 'beit.encoder.layer.19.attention.attention.query.weight', 'beit.encoder.layer.19.attention.attention.query.bias', 'beit.encoder.layer.19.attention.attention.key.weight', 'beit.encoder.layer.19.attention.attention.value.weight', 'beit.encoder.layer.19.attention.attention.value.bias', 'beit.encoder.layer.19.attention.output.dense.weight', 'beit.encoder.layer.19.attention.output.dense.bias', 'beit.encoder.layer.19.intermediate.dense.weight', 'beit.encoder.layer.19.intermediate.dense.bias', 'beit.encoder.layer.19.output.dense.weight', 'beit.encoder.layer.19.output.dense.bias', 'beit.encoder.layer.19.layernorm_before.weight', 'beit.encoder.layer.19.layernorm_before.bias', 'beit.encoder.layer.19.layernorm_after.weight', 'beit.encoder.layer.19.layernorm_after.bias', 'beit.encoder.layer.20.lambda_1', 'beit.encoder.layer.20.lambda_2', 'beit.encoder.layer.20.attention.attention.query.weight', 'beit.encoder.layer.20.attention.attention.query.bias', 'beit.encoder.layer.20.attention.attention.key.weight', 'beit.encoder.layer.20.attention.attention.value.weight', 'beit.encoder.layer.20.attention.attention.value.bias', 'beit.encoder.layer.20.attention.output.dense.weight', 'beit.encoder.layer.20.attention.output.dense.bias', 'beit.encoder.layer.20.intermediate.dense.weight', 'beit.encoder.layer.20.intermediate.dense.bias', 'beit.encoder.layer.20.output.dense.weight', 'beit.encoder.layer.20.output.dense.bias', 'beit.encoder.layer.20.layernorm_before.weight', 'beit.encoder.layer.20.layernorm_before.bias', 'beit.encoder.layer.20.layernorm_after.weight', 'beit.encoder.layer.20.layernorm_after.bias', 'beit.encoder.layer.21.lambda_1', 'beit.encoder.layer.21.lambda_2', 'beit.encoder.layer.21.attention.attention.query.weight', 'beit.encoder.layer.21.attention.attention.query.bias', 'beit.encoder.layer.21.attention.attention.key.weight', 'beit.encoder.layer.21.attention.attention.value.weight', 'beit.encoder.layer.21.attention.attention.value.bias', 'beit.encoder.layer.21.attention.output.dense.weight', 'beit.encoder.layer.21.attention.output.dense.bias', 'beit.encoder.layer.21.intermediate.dense.weight', 'beit.encoder.layer.21.intermediate.dense.bias', 'beit.encoder.layer.21.output.dense.weight', 'beit.encoder.layer.21.output.dense.bias', 'beit.encoder.layer.21.layernorm_before.weight', 'beit.encoder.layer.21.layernorm_before.bias', 'beit.encoder.layer.21.layernorm_after.weight', 'beit.encoder.layer.21.layernorm_after.bias', 'beit.encoder.layer.22.lambda_1', 'beit.encoder.layer.22.lambda_2', 'beit.encoder.layer.22.attention.attention.query.weight', 'beit.encoder.layer.22.attention.attention.query.bias', 'beit.encoder.layer.22.attention.attention.key.weight', 'beit.encoder.layer.22.attention.attention.value.weight', 'beit.encoder.layer.22.attention.attention.value.bias', 'beit.encoder.layer.22.attention.output.dense.weight', 'beit.encoder.layer.22.attention.output.dense.bias', 'beit.encoder.layer.22.intermediate.dense.weight', 'beit.encoder.layer.22.intermediate.dense.bias', 'beit.encoder.layer.22.output.dense.weight', 'beit.encoder.layer.22.output.dense.bias', 'beit.encoder.layer.22.layernorm_before.weight', 'beit.encoder.layer.22.layernorm_before.bias', 'beit.encoder.layer.22.layernorm_after.weight', 'beit.encoder.layer.22.layernorm_after.bias', 'beit.encoder.layer.23.lambda_1', 'beit.encoder.layer.23.lambda_2', 'beit.encoder.layer.23.attention.attention.query.weight', 'beit.encoder.layer.23.attention.attention.query.bias', 'beit.encoder.layer.23.attention.attention.key.weight', 'beit.encoder.layer.23.attention.attention.value.weight', 'beit.encoder.layer.23.attention.attention.value.bias', 'beit.encoder.layer.23.attention.output.dense.weight', 'beit.encoder.layer.23.attention.output.dense.bias', 'beit.encoder.layer.23.intermediate.dense.weight', 'beit.encoder.layer.23.intermediate.dense.bias', 'beit.encoder.layer.23.output.dense.weight', 'beit.encoder.layer.23.output.dense.bias', 'beit.encoder.layer.23.layernorm_before.weight', 'beit.encoder.layer.23.layernorm_before.bias', 'beit.encoder.layer.23.layernorm_after.weight', 'beit.encoder.layer.23.layernorm_after.bias', 'beit.pooler.layernorm.weight', 'beit.pooler.layernorm.bias', 'classifier.weight', 'classifier.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_20840\\2099681282.py:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='115' max='115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [115/115 01:13, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>0.631534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.700514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.562800</td>\n",
       "      <td>0.300123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.304500</td>\n",
       "      <td>0.285789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.212900</td>\n",
       "      <td>0.257943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8782\n",
      "AUC: 0.8264\n"
     ]
    }
   ],
   "source": [
    "T_and_T(breast_dataset_balanced_224, \"microsoft/beit-large-patch16-224-pt22k\", \"beit_breast_balanced_224\", 16, 0.1, True, True, beit_freezer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-large-patch16-224-pt22k and are newly initialized: ['beit.pooler.layernorm.bias', 'beit.pooler.layernorm.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_2964\\2099681282.py:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['beit.encoder.layer.19.lambda_1', 'beit.encoder.layer.19.lambda_2', 'beit.encoder.layer.19.attention.attention.query.weight', 'beit.encoder.layer.19.attention.attention.query.bias', 'beit.encoder.layer.19.attention.attention.key.weight', 'beit.encoder.layer.19.attention.attention.value.weight', 'beit.encoder.layer.19.attention.attention.value.bias', 'beit.encoder.layer.19.attention.output.dense.weight', 'beit.encoder.layer.19.attention.output.dense.bias', 'beit.encoder.layer.19.intermediate.dense.weight', 'beit.encoder.layer.19.intermediate.dense.bias', 'beit.encoder.layer.19.output.dense.weight', 'beit.encoder.layer.19.output.dense.bias', 'beit.encoder.layer.19.layernorm_before.weight', 'beit.encoder.layer.19.layernorm_before.bias', 'beit.encoder.layer.19.layernorm_after.weight', 'beit.encoder.layer.19.layernorm_after.bias', 'beit.encoder.layer.20.lambda_1', 'beit.encoder.layer.20.lambda_2', 'beit.encoder.layer.20.attention.attention.query.weight', 'beit.encoder.layer.20.attention.attention.query.bias', 'beit.encoder.layer.20.attention.attention.key.weight', 'beit.encoder.layer.20.attention.attention.value.weight', 'beit.encoder.layer.20.attention.attention.value.bias', 'beit.encoder.layer.20.attention.output.dense.weight', 'beit.encoder.layer.20.attention.output.dense.bias', 'beit.encoder.layer.20.intermediate.dense.weight', 'beit.encoder.layer.20.intermediate.dense.bias', 'beit.encoder.layer.20.output.dense.weight', 'beit.encoder.layer.20.output.dense.bias', 'beit.encoder.layer.20.layernorm_before.weight', 'beit.encoder.layer.20.layernorm_before.bias', 'beit.encoder.layer.20.layernorm_after.weight', 'beit.encoder.layer.20.layernorm_after.bias', 'beit.encoder.layer.21.lambda_1', 'beit.encoder.layer.21.lambda_2', 'beit.encoder.layer.21.attention.attention.query.weight', 'beit.encoder.layer.21.attention.attention.query.bias', 'beit.encoder.layer.21.attention.attention.key.weight', 'beit.encoder.layer.21.attention.attention.value.weight', 'beit.encoder.layer.21.attention.attention.value.bias', 'beit.encoder.layer.21.attention.output.dense.weight', 'beit.encoder.layer.21.attention.output.dense.bias', 'beit.encoder.layer.21.intermediate.dense.weight', 'beit.encoder.layer.21.intermediate.dense.bias', 'beit.encoder.layer.21.output.dense.weight', 'beit.encoder.layer.21.output.dense.bias', 'beit.encoder.layer.21.layernorm_before.weight', 'beit.encoder.layer.21.layernorm_before.bias', 'beit.encoder.layer.21.layernorm_after.weight', 'beit.encoder.layer.21.layernorm_after.bias', 'beit.encoder.layer.22.lambda_1', 'beit.encoder.layer.22.lambda_2', 'beit.encoder.layer.22.attention.attention.query.weight', 'beit.encoder.layer.22.attention.attention.query.bias', 'beit.encoder.layer.22.attention.attention.key.weight', 'beit.encoder.layer.22.attention.attention.value.weight', 'beit.encoder.layer.22.attention.attention.value.bias', 'beit.encoder.layer.22.attention.output.dense.weight', 'beit.encoder.layer.22.attention.output.dense.bias', 'beit.encoder.layer.22.intermediate.dense.weight', 'beit.encoder.layer.22.intermediate.dense.bias', 'beit.encoder.layer.22.output.dense.weight', 'beit.encoder.layer.22.output.dense.bias', 'beit.encoder.layer.22.layernorm_before.weight', 'beit.encoder.layer.22.layernorm_before.bias', 'beit.encoder.layer.22.layernorm_after.weight', 'beit.encoder.layer.22.layernorm_after.bias', 'beit.encoder.layer.23.lambda_1', 'beit.encoder.layer.23.lambda_2', 'beit.encoder.layer.23.attention.attention.query.weight', 'beit.encoder.layer.23.attention.attention.query.bias', 'beit.encoder.layer.23.attention.attention.key.weight', 'beit.encoder.layer.23.attention.attention.value.weight', 'beit.encoder.layer.23.attention.attention.value.bias', 'beit.encoder.layer.23.attention.output.dense.weight', 'beit.encoder.layer.23.attention.output.dense.bias', 'beit.encoder.layer.23.intermediate.dense.weight', 'beit.encoder.layer.23.intermediate.dense.bias', 'beit.encoder.layer.23.output.dense.weight', 'beit.encoder.layer.23.output.dense.bias', 'beit.encoder.layer.23.layernorm_before.weight', 'beit.encoder.layer.23.layernorm_before.bias', 'beit.encoder.layer.23.layernorm_after.weight', 'beit.encoder.layer.23.layernorm_after.bias', 'beit.pooler.layernorm.weight', 'beit.pooler.layernorm.bias', 'classifier.weight', 'classifier.bias']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='370' max='370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [370/370 10:25, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.259200</td>\n",
       "      <td>0.116012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.119900</td>\n",
       "      <td>0.145330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.075100</td>\n",
       "      <td>0.085481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>0.066247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>0.059150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8846\n",
      "AUC: 0.8487\n"
     ]
    }
   ],
   "source": [
    "T_and_T(pneumonia_dataset_28, \"microsoft/beit-large-patch16-224-pt22k\", \"beit_pneumonia_28\", 64, 0.1, True, True, beit_freezer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-large-patch16-224-pt22k and are newly initialized: ['beit.pooler.layernorm.bias', 'beit.pooler.layernorm.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['beit.encoder.layer.19.lambda_1', 'beit.encoder.layer.19.lambda_2', 'beit.encoder.layer.19.attention.attention.query.weight', 'beit.encoder.layer.19.attention.attention.query.bias', 'beit.encoder.layer.19.attention.attention.key.weight', 'beit.encoder.layer.19.attention.attention.value.weight', 'beit.encoder.layer.19.attention.attention.value.bias', 'beit.encoder.layer.19.attention.output.dense.weight', 'beit.encoder.layer.19.attention.output.dense.bias', 'beit.encoder.layer.19.intermediate.dense.weight', 'beit.encoder.layer.19.intermediate.dense.bias', 'beit.encoder.layer.19.output.dense.weight', 'beit.encoder.layer.19.output.dense.bias', 'beit.encoder.layer.19.layernorm_before.weight', 'beit.encoder.layer.19.layernorm_before.bias', 'beit.encoder.layer.19.layernorm_after.weight', 'beit.encoder.layer.19.layernorm_after.bias', 'beit.encoder.layer.20.lambda_1', 'beit.encoder.layer.20.lambda_2', 'beit.encoder.layer.20.attention.attention.query.weight', 'beit.encoder.layer.20.attention.attention.query.bias', 'beit.encoder.layer.20.attention.attention.key.weight', 'beit.encoder.layer.20.attention.attention.value.weight', 'beit.encoder.layer.20.attention.attention.value.bias', 'beit.encoder.layer.20.attention.output.dense.weight', 'beit.encoder.layer.20.attention.output.dense.bias', 'beit.encoder.layer.20.intermediate.dense.weight', 'beit.encoder.layer.20.intermediate.dense.bias', 'beit.encoder.layer.20.output.dense.weight', 'beit.encoder.layer.20.output.dense.bias', 'beit.encoder.layer.20.layernorm_before.weight', 'beit.encoder.layer.20.layernorm_before.bias', 'beit.encoder.layer.20.layernorm_after.weight', 'beit.encoder.layer.20.layernorm_after.bias', 'beit.encoder.layer.21.lambda_1', 'beit.encoder.layer.21.lambda_2', 'beit.encoder.layer.21.attention.attention.query.weight', 'beit.encoder.layer.21.attention.attention.query.bias', 'beit.encoder.layer.21.attention.attention.key.weight', 'beit.encoder.layer.21.attention.attention.value.weight', 'beit.encoder.layer.21.attention.attention.value.bias', 'beit.encoder.layer.21.attention.output.dense.weight', 'beit.encoder.layer.21.attention.output.dense.bias', 'beit.encoder.layer.21.intermediate.dense.weight', 'beit.encoder.layer.21.intermediate.dense.bias', 'beit.encoder.layer.21.output.dense.weight', 'beit.encoder.layer.21.output.dense.bias', 'beit.encoder.layer.21.layernorm_before.weight', 'beit.encoder.layer.21.layernorm_before.bias', 'beit.encoder.layer.21.layernorm_after.weight', 'beit.encoder.layer.21.layernorm_after.bias', 'beit.encoder.layer.22.lambda_1', 'beit.encoder.layer.22.lambda_2', 'beit.encoder.layer.22.attention.attention.query.weight', 'beit.encoder.layer.22.attention.attention.query.bias', 'beit.encoder.layer.22.attention.attention.key.weight', 'beit.encoder.layer.22.attention.attention.value.weight', 'beit.encoder.layer.22.attention.attention.value.bias', 'beit.encoder.layer.22.attention.output.dense.weight', 'beit.encoder.layer.22.attention.output.dense.bias', 'beit.encoder.layer.22.intermediate.dense.weight', 'beit.encoder.layer.22.intermediate.dense.bias', 'beit.encoder.layer.22.output.dense.weight', 'beit.encoder.layer.22.output.dense.bias', 'beit.encoder.layer.22.layernorm_before.weight', 'beit.encoder.layer.22.layernorm_before.bias', 'beit.encoder.layer.22.layernorm_after.weight', 'beit.encoder.layer.22.layernorm_after.bias', 'beit.encoder.layer.23.lambda_1', 'beit.encoder.layer.23.lambda_2', 'beit.encoder.layer.23.attention.attention.query.weight', 'beit.encoder.layer.23.attention.attention.query.bias', 'beit.encoder.layer.23.attention.attention.key.weight', 'beit.encoder.layer.23.attention.attention.value.weight', 'beit.encoder.layer.23.attention.attention.value.bias', 'beit.encoder.layer.23.attention.output.dense.weight', 'beit.encoder.layer.23.attention.output.dense.bias', 'beit.encoder.layer.23.intermediate.dense.weight', 'beit.encoder.layer.23.intermediate.dense.bias', 'beit.encoder.layer.23.output.dense.weight', 'beit.encoder.layer.23.output.dense.bias', 'beit.encoder.layer.23.layernorm_before.weight', 'beit.encoder.layer.23.layernorm_before.bias', 'beit.encoder.layer.23.layernorm_after.weight', 'beit.encoder.layer.23.layernorm_after.bias', 'beit.pooler.layernorm.weight', 'beit.pooler.layernorm.bias', 'classifier.weight', 'classifier.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_2964\\2099681282.py:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='475' max='475' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [475/475 07:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>0.222284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.107700</td>\n",
       "      <td>0.099022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.142600</td>\n",
       "      <td>0.081268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.061100</td>\n",
       "      <td>0.140866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.097864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9183\n",
      "AUC: 0.8962\n"
     ]
    }
   ],
   "source": [
    "T_and_T(pneumonia_dataset_balanced_28, \"microsoft/beit-large-patch16-224-pt22k\", \"beit_pneumonia_balanced_28\", 32, 0.1, True, True, beit_freezer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-large-patch16-224-pt22k and are newly initialized: ['beit.pooler.layernorm.bias', 'beit.pooler.layernorm.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['beit.encoder.layer.19.lambda_1', 'beit.encoder.layer.19.lambda_2', 'beit.encoder.layer.19.attention.attention.query.weight', 'beit.encoder.layer.19.attention.attention.query.bias', 'beit.encoder.layer.19.attention.attention.key.weight', 'beit.encoder.layer.19.attention.attention.value.weight', 'beit.encoder.layer.19.attention.attention.value.bias', 'beit.encoder.layer.19.attention.output.dense.weight', 'beit.encoder.layer.19.attention.output.dense.bias', 'beit.encoder.layer.19.intermediate.dense.weight', 'beit.encoder.layer.19.intermediate.dense.bias', 'beit.encoder.layer.19.output.dense.weight', 'beit.encoder.layer.19.output.dense.bias', 'beit.encoder.layer.19.layernorm_before.weight', 'beit.encoder.layer.19.layernorm_before.bias', 'beit.encoder.layer.19.layernorm_after.weight', 'beit.encoder.layer.19.layernorm_after.bias', 'beit.encoder.layer.20.lambda_1', 'beit.encoder.layer.20.lambda_2', 'beit.encoder.layer.20.attention.attention.query.weight', 'beit.encoder.layer.20.attention.attention.query.bias', 'beit.encoder.layer.20.attention.attention.key.weight', 'beit.encoder.layer.20.attention.attention.value.weight', 'beit.encoder.layer.20.attention.attention.value.bias', 'beit.encoder.layer.20.attention.output.dense.weight', 'beit.encoder.layer.20.attention.output.dense.bias', 'beit.encoder.layer.20.intermediate.dense.weight', 'beit.encoder.layer.20.intermediate.dense.bias', 'beit.encoder.layer.20.output.dense.weight', 'beit.encoder.layer.20.output.dense.bias', 'beit.encoder.layer.20.layernorm_before.weight', 'beit.encoder.layer.20.layernorm_before.bias', 'beit.encoder.layer.20.layernorm_after.weight', 'beit.encoder.layer.20.layernorm_after.bias', 'beit.encoder.layer.21.lambda_1', 'beit.encoder.layer.21.lambda_2', 'beit.encoder.layer.21.attention.attention.query.weight', 'beit.encoder.layer.21.attention.attention.query.bias', 'beit.encoder.layer.21.attention.attention.key.weight', 'beit.encoder.layer.21.attention.attention.value.weight', 'beit.encoder.layer.21.attention.attention.value.bias', 'beit.encoder.layer.21.attention.output.dense.weight', 'beit.encoder.layer.21.attention.output.dense.bias', 'beit.encoder.layer.21.intermediate.dense.weight', 'beit.encoder.layer.21.intermediate.dense.bias', 'beit.encoder.layer.21.output.dense.weight', 'beit.encoder.layer.21.output.dense.bias', 'beit.encoder.layer.21.layernorm_before.weight', 'beit.encoder.layer.21.layernorm_before.bias', 'beit.encoder.layer.21.layernorm_after.weight', 'beit.encoder.layer.21.layernorm_after.bias', 'beit.encoder.layer.22.lambda_1', 'beit.encoder.layer.22.lambda_2', 'beit.encoder.layer.22.attention.attention.query.weight', 'beit.encoder.layer.22.attention.attention.query.bias', 'beit.encoder.layer.22.attention.attention.key.weight', 'beit.encoder.layer.22.attention.attention.value.weight', 'beit.encoder.layer.22.attention.attention.value.bias', 'beit.encoder.layer.22.attention.output.dense.weight', 'beit.encoder.layer.22.attention.output.dense.bias', 'beit.encoder.layer.22.intermediate.dense.weight', 'beit.encoder.layer.22.intermediate.dense.bias', 'beit.encoder.layer.22.output.dense.weight', 'beit.encoder.layer.22.output.dense.bias', 'beit.encoder.layer.22.layernorm_before.weight', 'beit.encoder.layer.22.layernorm_before.bias', 'beit.encoder.layer.22.layernorm_after.weight', 'beit.encoder.layer.22.layernorm_after.bias', 'beit.encoder.layer.23.lambda_1', 'beit.encoder.layer.23.lambda_2', 'beit.encoder.layer.23.attention.attention.query.weight', 'beit.encoder.layer.23.attention.attention.query.bias', 'beit.encoder.layer.23.attention.attention.key.weight', 'beit.encoder.layer.23.attention.attention.value.weight', 'beit.encoder.layer.23.attention.attention.value.bias', 'beit.encoder.layer.23.attention.output.dense.weight', 'beit.encoder.layer.23.attention.output.dense.bias', 'beit.encoder.layer.23.intermediate.dense.weight', 'beit.encoder.layer.23.intermediate.dense.bias', 'beit.encoder.layer.23.output.dense.weight', 'beit.encoder.layer.23.output.dense.bias', 'beit.encoder.layer.23.layernorm_before.weight', 'beit.encoder.layer.23.layernorm_before.bias', 'beit.encoder.layer.23.layernorm_after.weight', 'beit.encoder.layer.23.layernorm_after.bias', 'beit.pooler.layernorm.weight', 'beit.pooler.layernorm.bias', 'classifier.weight', 'classifier.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_10660\\2099681282.py:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='740' max='740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [740/740 10:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.082900</td>\n",
       "      <td>0.078521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.038238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.039926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.043594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.031939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9038\n",
      "AUC: 0.8726\n"
     ]
    }
   ],
   "source": [
    "T_and_T(pneumonia_dataset_224, \"microsoft/beit-large-patch16-224-pt22k\", \"beit_pneumonia_224\", 32, 0.1, True, True, beit_freezer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-large-patch16-224-pt22k and are newly initialized: ['beit.pooler.layernorm.bias', 'beit.pooler.layernorm.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['beit.encoder.layer.19.lambda_1', 'beit.encoder.layer.19.lambda_2', 'beit.encoder.layer.19.attention.attention.query.weight', 'beit.encoder.layer.19.attention.attention.query.bias', 'beit.encoder.layer.19.attention.attention.key.weight', 'beit.encoder.layer.19.attention.attention.value.weight', 'beit.encoder.layer.19.attention.attention.value.bias', 'beit.encoder.layer.19.attention.output.dense.weight', 'beit.encoder.layer.19.attention.output.dense.bias', 'beit.encoder.layer.19.intermediate.dense.weight', 'beit.encoder.layer.19.intermediate.dense.bias', 'beit.encoder.layer.19.output.dense.weight', 'beit.encoder.layer.19.output.dense.bias', 'beit.encoder.layer.19.layernorm_before.weight', 'beit.encoder.layer.19.layernorm_before.bias', 'beit.encoder.layer.19.layernorm_after.weight', 'beit.encoder.layer.19.layernorm_after.bias', 'beit.encoder.layer.20.lambda_1', 'beit.encoder.layer.20.lambda_2', 'beit.encoder.layer.20.attention.attention.query.weight', 'beit.encoder.layer.20.attention.attention.query.bias', 'beit.encoder.layer.20.attention.attention.key.weight', 'beit.encoder.layer.20.attention.attention.value.weight', 'beit.encoder.layer.20.attention.attention.value.bias', 'beit.encoder.layer.20.attention.output.dense.weight', 'beit.encoder.layer.20.attention.output.dense.bias', 'beit.encoder.layer.20.intermediate.dense.weight', 'beit.encoder.layer.20.intermediate.dense.bias', 'beit.encoder.layer.20.output.dense.weight', 'beit.encoder.layer.20.output.dense.bias', 'beit.encoder.layer.20.layernorm_before.weight', 'beit.encoder.layer.20.layernorm_before.bias', 'beit.encoder.layer.20.layernorm_after.weight', 'beit.encoder.layer.20.layernorm_after.bias', 'beit.encoder.layer.21.lambda_1', 'beit.encoder.layer.21.lambda_2', 'beit.encoder.layer.21.attention.attention.query.weight', 'beit.encoder.layer.21.attention.attention.query.bias', 'beit.encoder.layer.21.attention.attention.key.weight', 'beit.encoder.layer.21.attention.attention.value.weight', 'beit.encoder.layer.21.attention.attention.value.bias', 'beit.encoder.layer.21.attention.output.dense.weight', 'beit.encoder.layer.21.attention.output.dense.bias', 'beit.encoder.layer.21.intermediate.dense.weight', 'beit.encoder.layer.21.intermediate.dense.bias', 'beit.encoder.layer.21.output.dense.weight', 'beit.encoder.layer.21.output.dense.bias', 'beit.encoder.layer.21.layernorm_before.weight', 'beit.encoder.layer.21.layernorm_before.bias', 'beit.encoder.layer.21.layernorm_after.weight', 'beit.encoder.layer.21.layernorm_after.bias', 'beit.encoder.layer.22.lambda_1', 'beit.encoder.layer.22.lambda_2', 'beit.encoder.layer.22.attention.attention.query.weight', 'beit.encoder.layer.22.attention.attention.query.bias', 'beit.encoder.layer.22.attention.attention.key.weight', 'beit.encoder.layer.22.attention.attention.value.weight', 'beit.encoder.layer.22.attention.attention.value.bias', 'beit.encoder.layer.22.attention.output.dense.weight', 'beit.encoder.layer.22.attention.output.dense.bias', 'beit.encoder.layer.22.intermediate.dense.weight', 'beit.encoder.layer.22.intermediate.dense.bias', 'beit.encoder.layer.22.output.dense.weight', 'beit.encoder.layer.22.output.dense.bias', 'beit.encoder.layer.22.layernorm_before.weight', 'beit.encoder.layer.22.layernorm_before.bias', 'beit.encoder.layer.22.layernorm_after.weight', 'beit.encoder.layer.22.layernorm_after.bias', 'beit.encoder.layer.23.lambda_1', 'beit.encoder.layer.23.lambda_2', 'beit.encoder.layer.23.attention.attention.query.weight', 'beit.encoder.layer.23.attention.attention.query.bias', 'beit.encoder.layer.23.attention.attention.key.weight', 'beit.encoder.layer.23.attention.attention.value.weight', 'beit.encoder.layer.23.attention.attention.value.bias', 'beit.encoder.layer.23.attention.output.dense.weight', 'beit.encoder.layer.23.attention.output.dense.bias', 'beit.encoder.layer.23.intermediate.dense.weight', 'beit.encoder.layer.23.intermediate.dense.bias', 'beit.encoder.layer.23.output.dense.weight', 'beit.encoder.layer.23.output.dense.bias', 'beit.encoder.layer.23.layernorm_before.weight', 'beit.encoder.layer.23.layernorm_before.bias', 'beit.encoder.layer.23.layernorm_after.weight', 'beit.encoder.layer.23.layernorm_after.bias', 'beit.pooler.layernorm.weight', 'beit.pooler.layernorm.bias', 'classifier.weight', 'classifier.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_20840\\2099681282.py:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='475' max='475' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [475/475 07:23, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.106134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>0.054539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.060100</td>\n",
       "      <td>0.050496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.061132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.052801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9231\n",
      "AUC: 0.8991\n"
     ]
    }
   ],
   "source": [
    "T_and_T(pneumonia_dataset_balanced_224, \"microsoft/beit-large-patch16-224-pt22k\", \"beit_pneumonia_balanced_224\", 32, 0.1, True, True, beit_freezer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
