{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from datasets import Dataset as HFDataset, load_dataset, DatasetDict\n",
    "from typing import Optional, Tuple, Dict, Any\n",
    "from medmnist import BreastMNIST, PneumoniaMNIST\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, recall_score, roc_auc_score\n",
    "import random\n",
    "from imblearn.under_sampling import NearMiss\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.utils import resample\n",
    "import datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedMNISTtoHF(Dataset):\n",
    "    def __init__(self, medmnist_dataset):\n",
    "        \"\"\"\n",
    "        Convert MedMNIST dataset to a format compatible with HuggingFace models\n",
    "        Args:\n",
    "            medmnist_dataset: The original MedMNIST dataset\n",
    "            transform: Optional transforms to be applied to the images\n",
    "        \"\"\"\n",
    "        self.dataset = medmnist_dataset\n",
    "        \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        img, label = self.dataset[idx]\n",
    "\n",
    "        return {\n",
    "            \"image\": img,\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def convert_medmnist_to_hf(medmnist_dataset):\n",
    "    \"\"\"\n",
    "    Convert a MedMNIST dataset to a HuggingFace dataset\n",
    "    Args:\n",
    "        medmnist_dataset: The original MedMNIST dataset\n",
    "    Returns:\n",
    "        The HuggingFace dataset\n",
    "    \"\"\"\n",
    "    # Create wrapper dataset\n",
    "    wrapper_dataset = MedMNISTtoHF(medmnist_dataset)\n",
    "    \n",
    "    # Convert to HF format\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(len(wrapper_dataset)):\n",
    "        sample = wrapper_dataset[i]\n",
    "        images.append(sample[\"image\"])\n",
    "        labels.append(sample[\"label\"].item())\n",
    "    \n",
    "    # Create HF dataset\n",
    "    hf_dataset = HFDataset.from_dict({\n",
    "        \"image\": images,\n",
    "        \"label\": labels\n",
    "    })\n",
    "    \n",
    "    return hf_dataset\n",
    "\n",
    "def dataset_balancing(dataset, alpha):\n",
    "    \"\"\"\t\n",
    "    Balance the dataset by oversampling the minority class\n",
    "    Args:\n",
    "        dataset: The original dataset\n",
    "        alpha: The oversampling factor\n",
    "    Returns:\n",
    "        The balanced dataset\n",
    "    \"\"\"\n",
    "\n",
    "    mj_class = Counter(dataset['label']).most_common(1)[0][0]\n",
    "    mn_class = abs(mj_class-1)\n",
    "    data = dataset['image']\n",
    "\n",
    "    mask = [lb== mj_class for lb in dataset['label']]\n",
    "    X_majority = [img for img,flag in zip(data, mask) if flag]\n",
    "    X_minority = [img for img,flag in zip(data, mask) if not flag]\n",
    "    new_len_majority = len(X_minority) + int(alpha*len(X_minority))\n",
    "    X_majority_resampled = resample(X_majority, \n",
    "                                    replace=False,  # No replacement\n",
    "                                    n_samples=new_len_majority,  # Match minority class size\n",
    "                                    random_state=42)\n",
    "    X_resampled = X_majority_resampled + X_minority\n",
    "    y_resampled = [mj_class]*new_len_majority + [mn_class]*len(X_minority)\n",
    "    random.seed(42)\n",
    "    random.shuffle(X_resampled)\n",
    "    random.seed(42)\n",
    "    random.shuffle(y_resampled)\n",
    "\n",
    "    dict_blanced_dataset = {\n",
    "        \"image\": X_resampled,\n",
    "        \"label\": y_resampled\n",
    "    }\n",
    "    balanced_dataset = datasets.Dataset.from_dict(dict_blanced_dataset)\n",
    "    return balanced_dataset\n",
    "\n",
    "def load_dataset_medmnist(dataset_name, size, balancing):\n",
    "    \"\"\"\n",
    "    Load a MedMNIST dataset and convert it to a HuggingFace dataset\n",
    "    Args:\n",
    "        dataset_name: The name of the MedMNIST dataset\n",
    "        size: The size of the images\n",
    "        balancing: Whether to balance the dataset\n",
    "    Returns:\n",
    "        The HuggingFace dataset\n",
    "    \"\"\"\n",
    "    # Load MedMNIST dataset\n",
    "    train_dataset = dataset_name(split='train', download=True, size=size)\n",
    "    val_dataset = dataset_name(split='val', download=True, size=size)\n",
    "    test_dataset = dataset_name(split='test', download=True, size=size)\n",
    "\n",
    "    # Convert to HuggingFace dataset\n",
    "    hf_train_dataset = convert_medmnist_to_hf(train_dataset)\n",
    "    hf_val_dataset = convert_medmnist_to_hf(val_dataset)\n",
    "    hf_test_dataset = convert_medmnist_to_hf(test_dataset)\n",
    "\n",
    "    # Balancing\n",
    "    if balancing:\n",
    "        hf_train_dataset_balanced = dataset_balancing(hf_train_dataset, 0.5)\n",
    "        dataset = DatasetDict({\"train\": hf_train_dataset_balanced, \"validation\": hf_val_dataset, \"test\": hf_test_dataset})\n",
    "        return dataset\n",
    "    else:\n",
    "        dataset = DatasetDict({\"train\": hf_train_dataset, \"validation\": hf_val_dataset, \"test\": hf_test_dataset})\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist_224.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist_224.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist_224.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist_224.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist_224.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\breastmnist_224.npz\n",
      "Downloading https://zenodo.org/records/10519652/files/pneumoniamnist.npz?download=1 to C:\\Users\\baiet\\.medmnist\\pneumoniamnist.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.17M/4.17M [00:00<00:00, 5.59MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\pneumoniamnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\pneumoniamnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\pneumoniamnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\pneumoniamnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\pneumoniamnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\pneumoniamnist_224.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\pneumoniamnist_224.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\pneumoniamnist_224.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\pneumoniamnist_224.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\pneumoniamnist_224.npz\n",
      "Using downloaded and verified file: C:\\Users\\baiet\\.medmnist\\pneumoniamnist_224.npz\n"
     ]
    }
   ],
   "source": [
    "breast_dataset_28 = load_dataset_medmnist(BreastMNIST, 28, False)\n",
    "breast_dataset_balanced_28 = load_dataset_medmnist(BreastMNIST, 28, True)\n",
    "breast_dataset_224 = load_dataset_medmnist(BreastMNIST, 224, False)\n",
    "breast_dataset_balanced_224 = load_dataset_medmnist(BreastMNIST, 224, True)\n",
    "# label = 1 => no disease\n",
    "\n",
    "pneumonia_dataset_28 = load_dataset_medmnist(PneumoniaMNIST, 28, False)\n",
    "pneumonia_dataset_balanced_28 = load_dataset_medmnist(PneumoniaMNIST, 28, True)\n",
    "pneumonia_dataset_224 = load_dataset_medmnist(PneumoniaMNIST, 224, False)\n",
    "pneumonia_dataset_balanced_224 = load_dataset_medmnist(PneumoniaMNIST, 224, True)\n",
    "# label = 1 => with disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freezer Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beit_freezer(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(\"classifier\") \\\n",
    "            and not name.startswith(\"beit.pooler\")\\\n",
    "            and not name.startswith(\"beit.encoder.layer.23\")\\\n",
    "            and not name.startswith(\"beit.encoder.layer.22\")\\\n",
    "            and not name.startswith(\"beit.encoder.layer.21\")\\\n",
    "            and not name.startswith(\"beit.encoder.layer.20\")\\\n",
    "            and not name.startswith(\"beit.encoder.layer.19\"):\n",
    "            param.requires_grad = False\n",
    "\n",
    "def resnet_freezer(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(\"classifier\")\\\n",
    "            and not name.startswith(\"resnet.encoder.stages.3.layers.2\"):\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_reproducibility(seed=42):\n",
    "    # Set seeds for reproducibility\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # For multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # Turn off to ensure deterministic results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T_and_T(dataset, model_name, output_dir, batch_size, weight_decay, Training, Testing, freezer):\n",
    "    # Set Seed\n",
    "    set_reproducibility()\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load the BEiT-large model and image processor\n",
    "    model = AutoModelForImageClassification.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True).to(device) \n",
    "    processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "    # Freeze all layers except the classifier and the last transformer layer\n",
    "    freezer(model)\n",
    "\n",
    "    # Verify which layers are trainable\n",
    "    trainable_params = [name for name, param in model.named_parameters() if param.requires_grad]\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "    # Define preprocessing function\n",
    "    def preprocess_images(examples):\n",
    "            images = [processor(image.convert(\"RGB\"), return_tensors=\"pt\") for image in examples[\"image\"]]\n",
    "            pixel_values = torch.stack([image[\"pixel_values\"].squeeze() for image in images])\n",
    "            labels = torch.tensor(examples[\"label\"])\n",
    "            return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    train_dataset = dataset[\"train\"].with_transform(preprocess_images)\n",
    "    validation_dataset = dataset[\"validation\"].with_transform(preprocess_images)\n",
    "    test_dataset = dataset[\"test\"].with_transform(preprocess_images)\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=3e-4,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=weight_decay,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        save_total_limit=2,\n",
    "        remove_unused_columns=False,\n",
    "        push_to_hub=False,\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    # Define Trainer\n",
    "    trainer_beit = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset,\n",
    "        tokenizer=processor,\n",
    "    )\n",
    "    if Training:\n",
    "        trainer_beit.train()\n",
    "        model.save_pretrained(output_dir)\n",
    "        processor.save_pretrained(output_dir)\n",
    "\n",
    "    if Testing:\n",
    "        predictions = trainer_beit.predict(test_dataset)\n",
    "        pred_labels = predictions.predictions.argmax(axis=1)\n",
    "        true_labels = predictions.label_ids\n",
    "\n",
    "        # Calculate additional metrics\n",
    "        accuracy = accuracy_score(true_labels, pred_labels)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average=\"binary\")\n",
    "        specificity = recall_score(true_labels, pred_labels, pos_label=0)\n",
    "        auc = roc_auc_score(true_labels, pred_labels)\n",
    "\n",
    "        # Display the metrics\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision (weighted): {precision:.4f}\")\n",
    "        print(f\"Recall (weighted): {recall:.4f}\")\n",
    "        print(f\"F1-Score (weighted): {f1:.4f}\")\n",
    "        print(f\"Specificity: {specificity:.4f}\")\n",
    "        print(f\"AUC: {auc:.4f}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-large-patch16-224-pt22k and are newly initialized: ['beit.pooler.layernorm.bias', 'beit.pooler.layernorm.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['beit.encoder.layer.19.lambda_1', 'beit.encoder.layer.19.lambda_2', 'beit.encoder.layer.19.attention.attention.query.weight', 'beit.encoder.layer.19.attention.attention.query.bias', 'beit.encoder.layer.19.attention.attention.key.weight', 'beit.encoder.layer.19.attention.attention.value.weight', 'beit.encoder.layer.19.attention.attention.value.bias', 'beit.encoder.layer.19.attention.output.dense.weight', 'beit.encoder.layer.19.attention.output.dense.bias', 'beit.encoder.layer.19.intermediate.dense.weight', 'beit.encoder.layer.19.intermediate.dense.bias', 'beit.encoder.layer.19.output.dense.weight', 'beit.encoder.layer.19.output.dense.bias', 'beit.encoder.layer.19.layernorm_before.weight', 'beit.encoder.layer.19.layernorm_before.bias', 'beit.encoder.layer.19.layernorm_after.weight', 'beit.encoder.layer.19.layernorm_after.bias', 'beit.encoder.layer.20.lambda_1', 'beit.encoder.layer.20.lambda_2', 'beit.encoder.layer.20.attention.attention.query.weight', 'beit.encoder.layer.20.attention.attention.query.bias', 'beit.encoder.layer.20.attention.attention.key.weight', 'beit.encoder.layer.20.attention.attention.value.weight', 'beit.encoder.layer.20.attention.attention.value.bias', 'beit.encoder.layer.20.attention.output.dense.weight', 'beit.encoder.layer.20.attention.output.dense.bias', 'beit.encoder.layer.20.intermediate.dense.weight', 'beit.encoder.layer.20.intermediate.dense.bias', 'beit.encoder.layer.20.output.dense.weight', 'beit.encoder.layer.20.output.dense.bias', 'beit.encoder.layer.20.layernorm_before.weight', 'beit.encoder.layer.20.layernorm_before.bias', 'beit.encoder.layer.20.layernorm_after.weight', 'beit.encoder.layer.20.layernorm_after.bias', 'beit.encoder.layer.21.lambda_1', 'beit.encoder.layer.21.lambda_2', 'beit.encoder.layer.21.attention.attention.query.weight', 'beit.encoder.layer.21.attention.attention.query.bias', 'beit.encoder.layer.21.attention.attention.key.weight', 'beit.encoder.layer.21.attention.attention.value.weight', 'beit.encoder.layer.21.attention.attention.value.bias', 'beit.encoder.layer.21.attention.output.dense.weight', 'beit.encoder.layer.21.attention.output.dense.bias', 'beit.encoder.layer.21.intermediate.dense.weight', 'beit.encoder.layer.21.intermediate.dense.bias', 'beit.encoder.layer.21.output.dense.weight', 'beit.encoder.layer.21.output.dense.bias', 'beit.encoder.layer.21.layernorm_before.weight', 'beit.encoder.layer.21.layernorm_before.bias', 'beit.encoder.layer.21.layernorm_after.weight', 'beit.encoder.layer.21.layernorm_after.bias', 'beit.encoder.layer.22.lambda_1', 'beit.encoder.layer.22.lambda_2', 'beit.encoder.layer.22.attention.attention.query.weight', 'beit.encoder.layer.22.attention.attention.query.bias', 'beit.encoder.layer.22.attention.attention.key.weight', 'beit.encoder.layer.22.attention.attention.value.weight', 'beit.encoder.layer.22.attention.attention.value.bias', 'beit.encoder.layer.22.attention.output.dense.weight', 'beit.encoder.layer.22.attention.output.dense.bias', 'beit.encoder.layer.22.intermediate.dense.weight', 'beit.encoder.layer.22.intermediate.dense.bias', 'beit.encoder.layer.22.output.dense.weight', 'beit.encoder.layer.22.output.dense.bias', 'beit.encoder.layer.22.layernorm_before.weight', 'beit.encoder.layer.22.layernorm_before.bias', 'beit.encoder.layer.22.layernorm_after.weight', 'beit.encoder.layer.22.layernorm_after.bias', 'beit.encoder.layer.23.lambda_1', 'beit.encoder.layer.23.lambda_2', 'beit.encoder.layer.23.attention.attention.query.weight', 'beit.encoder.layer.23.attention.attention.query.bias', 'beit.encoder.layer.23.attention.attention.key.weight', 'beit.encoder.layer.23.attention.attention.value.weight', 'beit.encoder.layer.23.attention.attention.value.bias', 'beit.encoder.layer.23.attention.output.dense.weight', 'beit.encoder.layer.23.attention.output.dense.bias', 'beit.encoder.layer.23.intermediate.dense.weight', 'beit.encoder.layer.23.intermediate.dense.bias', 'beit.encoder.layer.23.output.dense.weight', 'beit.encoder.layer.23.output.dense.bias', 'beit.encoder.layer.23.layernorm_before.weight', 'beit.encoder.layer.23.layernorm_before.bias', 'beit.encoder.layer.23.layernorm_after.weight', 'beit.encoder.layer.23.layernorm_after.bias', 'beit.pooler.layernorm.weight', 'beit.pooler.layernorm.bias', 'classifier.weight', 'classifier.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_4812\\2090117231.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_beit = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [175/175 02:08, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.581400</td>\n",
       "      <td>0.532466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.545700</td>\n",
       "      <td>0.373951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.425400</td>\n",
       "      <td>0.309652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.230200</td>\n",
       "      <td>0.303705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.239300</td>\n",
       "      <td>0.264146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8910\n",
      "Precision (weighted): 0.9008\n",
      "Recall (weighted): 0.9561\n",
      "F1-Score (weighted): 0.9277\n",
      "Specificity: 0.7143\n",
      "AUC: 0.8352\n"
     ]
    }
   ],
   "source": [
    "T_and_T(breast_dataset_28, \"microsoft/beit-large-patch16-224-pt22k\", \"beit_breast_28\", 16, 0.01, True, True, beit_freezer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-large-patch16-224-pt22k and are newly initialized: ['beit.pooler.layernorm.bias', 'beit.pooler.layernorm.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['beit.encoder.layer.19.lambda_1', 'beit.encoder.layer.19.lambda_2', 'beit.encoder.layer.19.attention.attention.query.weight', 'beit.encoder.layer.19.attention.attention.query.bias', 'beit.encoder.layer.19.attention.attention.key.weight', 'beit.encoder.layer.19.attention.attention.value.weight', 'beit.encoder.layer.19.attention.attention.value.bias', 'beit.encoder.layer.19.attention.output.dense.weight', 'beit.encoder.layer.19.attention.output.dense.bias', 'beit.encoder.layer.19.intermediate.dense.weight', 'beit.encoder.layer.19.intermediate.dense.bias', 'beit.encoder.layer.19.output.dense.weight', 'beit.encoder.layer.19.output.dense.bias', 'beit.encoder.layer.19.layernorm_before.weight', 'beit.encoder.layer.19.layernorm_before.bias', 'beit.encoder.layer.19.layernorm_after.weight', 'beit.encoder.layer.19.layernorm_after.bias', 'beit.encoder.layer.20.lambda_1', 'beit.encoder.layer.20.lambda_2', 'beit.encoder.layer.20.attention.attention.query.weight', 'beit.encoder.layer.20.attention.attention.query.bias', 'beit.encoder.layer.20.attention.attention.key.weight', 'beit.encoder.layer.20.attention.attention.value.weight', 'beit.encoder.layer.20.attention.attention.value.bias', 'beit.encoder.layer.20.attention.output.dense.weight', 'beit.encoder.layer.20.attention.output.dense.bias', 'beit.encoder.layer.20.intermediate.dense.weight', 'beit.encoder.layer.20.intermediate.dense.bias', 'beit.encoder.layer.20.output.dense.weight', 'beit.encoder.layer.20.output.dense.bias', 'beit.encoder.layer.20.layernorm_before.weight', 'beit.encoder.layer.20.layernorm_before.bias', 'beit.encoder.layer.20.layernorm_after.weight', 'beit.encoder.layer.20.layernorm_after.bias', 'beit.encoder.layer.21.lambda_1', 'beit.encoder.layer.21.lambda_2', 'beit.encoder.layer.21.attention.attention.query.weight', 'beit.encoder.layer.21.attention.attention.query.bias', 'beit.encoder.layer.21.attention.attention.key.weight', 'beit.encoder.layer.21.attention.attention.value.weight', 'beit.encoder.layer.21.attention.attention.value.bias', 'beit.encoder.layer.21.attention.output.dense.weight', 'beit.encoder.layer.21.attention.output.dense.bias', 'beit.encoder.layer.21.intermediate.dense.weight', 'beit.encoder.layer.21.intermediate.dense.bias', 'beit.encoder.layer.21.output.dense.weight', 'beit.encoder.layer.21.output.dense.bias', 'beit.encoder.layer.21.layernorm_before.weight', 'beit.encoder.layer.21.layernorm_before.bias', 'beit.encoder.layer.21.layernorm_after.weight', 'beit.encoder.layer.21.layernorm_after.bias', 'beit.encoder.layer.22.lambda_1', 'beit.encoder.layer.22.lambda_2', 'beit.encoder.layer.22.attention.attention.query.weight', 'beit.encoder.layer.22.attention.attention.query.bias', 'beit.encoder.layer.22.attention.attention.key.weight', 'beit.encoder.layer.22.attention.attention.value.weight', 'beit.encoder.layer.22.attention.attention.value.bias', 'beit.encoder.layer.22.attention.output.dense.weight', 'beit.encoder.layer.22.attention.output.dense.bias', 'beit.encoder.layer.22.intermediate.dense.weight', 'beit.encoder.layer.22.intermediate.dense.bias', 'beit.encoder.layer.22.output.dense.weight', 'beit.encoder.layer.22.output.dense.bias', 'beit.encoder.layer.22.layernorm_before.weight', 'beit.encoder.layer.22.layernorm_before.bias', 'beit.encoder.layer.22.layernorm_after.weight', 'beit.encoder.layer.22.layernorm_after.bias', 'beit.encoder.layer.23.lambda_1', 'beit.encoder.layer.23.lambda_2', 'beit.encoder.layer.23.attention.attention.query.weight', 'beit.encoder.layer.23.attention.attention.query.bias', 'beit.encoder.layer.23.attention.attention.key.weight', 'beit.encoder.layer.23.attention.attention.value.weight', 'beit.encoder.layer.23.attention.attention.value.bias', 'beit.encoder.layer.23.attention.output.dense.weight', 'beit.encoder.layer.23.attention.output.dense.bias', 'beit.encoder.layer.23.intermediate.dense.weight', 'beit.encoder.layer.23.intermediate.dense.bias', 'beit.encoder.layer.23.output.dense.weight', 'beit.encoder.layer.23.output.dense.bias', 'beit.encoder.layer.23.layernorm_before.weight', 'beit.encoder.layer.23.layernorm_before.bias', 'beit.encoder.layer.23.layernorm_after.weight', 'beit.encoder.layer.23.layernorm_after.bias', 'beit.pooler.layernorm.weight', 'beit.pooler.layernorm.bias', 'classifier.weight', 'classifier.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_4812\\2090117231.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_beit = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='115' max='115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [115/115 01:28, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.682800</td>\n",
       "      <td>0.711752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.628900</td>\n",
       "      <td>0.442710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.592400</td>\n",
       "      <td>0.359735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.451900</td>\n",
       "      <td>0.278309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.373300</td>\n",
       "      <td>0.277980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8846\n",
      "Precision (weighted): 0.9138\n",
      "Recall (weighted): 0.9298\n",
      "F1-Score (weighted): 0.9217\n",
      "Specificity: 0.7619\n",
      "AUC: 0.8459\n"
     ]
    }
   ],
   "source": [
    "T_and_T(breast_dataset_balanced_28, \"microsoft/beit-large-patch16-224-pt22k\", \"beit_breast_balanced_28\", 16, 0, True, True, beit_freezer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-large-patch16-224-pt22k and are newly initialized: ['beit.pooler.layernorm.bias', 'beit.pooler.layernorm.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['beit.encoder.layer.19.lambda_1', 'beit.encoder.layer.19.lambda_2', 'beit.encoder.layer.19.attention.attention.query.weight', 'beit.encoder.layer.19.attention.attention.query.bias', 'beit.encoder.layer.19.attention.attention.key.weight', 'beit.encoder.layer.19.attention.attention.value.weight', 'beit.encoder.layer.19.attention.attention.value.bias', 'beit.encoder.layer.19.attention.output.dense.weight', 'beit.encoder.layer.19.attention.output.dense.bias', 'beit.encoder.layer.19.intermediate.dense.weight', 'beit.encoder.layer.19.intermediate.dense.bias', 'beit.encoder.layer.19.output.dense.weight', 'beit.encoder.layer.19.output.dense.bias', 'beit.encoder.layer.19.layernorm_before.weight', 'beit.encoder.layer.19.layernorm_before.bias', 'beit.encoder.layer.19.layernorm_after.weight', 'beit.encoder.layer.19.layernorm_after.bias', 'beit.encoder.layer.20.lambda_1', 'beit.encoder.layer.20.lambda_2', 'beit.encoder.layer.20.attention.attention.query.weight', 'beit.encoder.layer.20.attention.attention.query.bias', 'beit.encoder.layer.20.attention.attention.key.weight', 'beit.encoder.layer.20.attention.attention.value.weight', 'beit.encoder.layer.20.attention.attention.value.bias', 'beit.encoder.layer.20.attention.output.dense.weight', 'beit.encoder.layer.20.attention.output.dense.bias', 'beit.encoder.layer.20.intermediate.dense.weight', 'beit.encoder.layer.20.intermediate.dense.bias', 'beit.encoder.layer.20.output.dense.weight', 'beit.encoder.layer.20.output.dense.bias', 'beit.encoder.layer.20.layernorm_before.weight', 'beit.encoder.layer.20.layernorm_before.bias', 'beit.encoder.layer.20.layernorm_after.weight', 'beit.encoder.layer.20.layernorm_after.bias', 'beit.encoder.layer.21.lambda_1', 'beit.encoder.layer.21.lambda_2', 'beit.encoder.layer.21.attention.attention.query.weight', 'beit.encoder.layer.21.attention.attention.query.bias', 'beit.encoder.layer.21.attention.attention.key.weight', 'beit.encoder.layer.21.attention.attention.value.weight', 'beit.encoder.layer.21.attention.attention.value.bias', 'beit.encoder.layer.21.attention.output.dense.weight', 'beit.encoder.layer.21.attention.output.dense.bias', 'beit.encoder.layer.21.intermediate.dense.weight', 'beit.encoder.layer.21.intermediate.dense.bias', 'beit.encoder.layer.21.output.dense.weight', 'beit.encoder.layer.21.output.dense.bias', 'beit.encoder.layer.21.layernorm_before.weight', 'beit.encoder.layer.21.layernorm_before.bias', 'beit.encoder.layer.21.layernorm_after.weight', 'beit.encoder.layer.21.layernorm_after.bias', 'beit.encoder.layer.22.lambda_1', 'beit.encoder.layer.22.lambda_2', 'beit.encoder.layer.22.attention.attention.query.weight', 'beit.encoder.layer.22.attention.attention.query.bias', 'beit.encoder.layer.22.attention.attention.key.weight', 'beit.encoder.layer.22.attention.attention.value.weight', 'beit.encoder.layer.22.attention.attention.value.bias', 'beit.encoder.layer.22.attention.output.dense.weight', 'beit.encoder.layer.22.attention.output.dense.bias', 'beit.encoder.layer.22.intermediate.dense.weight', 'beit.encoder.layer.22.intermediate.dense.bias', 'beit.encoder.layer.22.output.dense.weight', 'beit.encoder.layer.22.output.dense.bias', 'beit.encoder.layer.22.layernorm_before.weight', 'beit.encoder.layer.22.layernorm_before.bias', 'beit.encoder.layer.22.layernorm_after.weight', 'beit.encoder.layer.22.layernorm_after.bias', 'beit.encoder.layer.23.lambda_1', 'beit.encoder.layer.23.lambda_2', 'beit.encoder.layer.23.attention.attention.query.weight', 'beit.encoder.layer.23.attention.attention.query.bias', 'beit.encoder.layer.23.attention.attention.key.weight', 'beit.encoder.layer.23.attention.attention.value.weight', 'beit.encoder.layer.23.attention.attention.value.bias', 'beit.encoder.layer.23.attention.output.dense.weight', 'beit.encoder.layer.23.attention.output.dense.bias', 'beit.encoder.layer.23.intermediate.dense.weight', 'beit.encoder.layer.23.intermediate.dense.bias', 'beit.encoder.layer.23.output.dense.weight', 'beit.encoder.layer.23.output.dense.bias', 'beit.encoder.layer.23.layernorm_before.weight', 'beit.encoder.layer.23.layernorm_before.bias', 'beit.encoder.layer.23.layernorm_after.weight', 'beit.encoder.layer.23.layernorm_after.bias', 'beit.pooler.layernorm.weight', 'beit.pooler.layernorm.bias', 'classifier.weight', 'classifier.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_4812\\2090117231.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_beit = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [175/175 02:25, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.581100</td>\n",
       "      <td>0.421221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.501900</td>\n",
       "      <td>0.303186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.403800</td>\n",
       "      <td>0.276382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.122300</td>\n",
       "      <td>0.250387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.155500</td>\n",
       "      <td>0.295957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8782\n",
      "Precision (weighted): 0.8740\n",
      "Recall (weighted): 0.9737\n",
      "F1-Score (weighted): 0.9212\n",
      "Specificity: 0.6190\n",
      "AUC: 0.7964\n"
     ]
    }
   ],
   "source": [
    "T_and_T(breast_dataset_224, \"microsoft/beit-large-patch16-224-pt22k\", \"beit_breast_224\", 16, 0.1, True, True, beit_freezer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-large-patch16-224-pt22k and are newly initialized: ['beit.pooler.layernorm.bias', 'beit.pooler.layernorm.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['beit.encoder.layer.19.lambda_1', 'beit.encoder.layer.19.lambda_2', 'beit.encoder.layer.19.attention.attention.query.weight', 'beit.encoder.layer.19.attention.attention.query.bias', 'beit.encoder.layer.19.attention.attention.key.weight', 'beit.encoder.layer.19.attention.attention.value.weight', 'beit.encoder.layer.19.attention.attention.value.bias', 'beit.encoder.layer.19.attention.output.dense.weight', 'beit.encoder.layer.19.attention.output.dense.bias', 'beit.encoder.layer.19.intermediate.dense.weight', 'beit.encoder.layer.19.intermediate.dense.bias', 'beit.encoder.layer.19.output.dense.weight', 'beit.encoder.layer.19.output.dense.bias', 'beit.encoder.layer.19.layernorm_before.weight', 'beit.encoder.layer.19.layernorm_before.bias', 'beit.encoder.layer.19.layernorm_after.weight', 'beit.encoder.layer.19.layernorm_after.bias', 'beit.encoder.layer.20.lambda_1', 'beit.encoder.layer.20.lambda_2', 'beit.encoder.layer.20.attention.attention.query.weight', 'beit.encoder.layer.20.attention.attention.query.bias', 'beit.encoder.layer.20.attention.attention.key.weight', 'beit.encoder.layer.20.attention.attention.value.weight', 'beit.encoder.layer.20.attention.attention.value.bias', 'beit.encoder.layer.20.attention.output.dense.weight', 'beit.encoder.layer.20.attention.output.dense.bias', 'beit.encoder.layer.20.intermediate.dense.weight', 'beit.encoder.layer.20.intermediate.dense.bias', 'beit.encoder.layer.20.output.dense.weight', 'beit.encoder.layer.20.output.dense.bias', 'beit.encoder.layer.20.layernorm_before.weight', 'beit.encoder.layer.20.layernorm_before.bias', 'beit.encoder.layer.20.layernorm_after.weight', 'beit.encoder.layer.20.layernorm_after.bias', 'beit.encoder.layer.21.lambda_1', 'beit.encoder.layer.21.lambda_2', 'beit.encoder.layer.21.attention.attention.query.weight', 'beit.encoder.layer.21.attention.attention.query.bias', 'beit.encoder.layer.21.attention.attention.key.weight', 'beit.encoder.layer.21.attention.attention.value.weight', 'beit.encoder.layer.21.attention.attention.value.bias', 'beit.encoder.layer.21.attention.output.dense.weight', 'beit.encoder.layer.21.attention.output.dense.bias', 'beit.encoder.layer.21.intermediate.dense.weight', 'beit.encoder.layer.21.intermediate.dense.bias', 'beit.encoder.layer.21.output.dense.weight', 'beit.encoder.layer.21.output.dense.bias', 'beit.encoder.layer.21.layernorm_before.weight', 'beit.encoder.layer.21.layernorm_before.bias', 'beit.encoder.layer.21.layernorm_after.weight', 'beit.encoder.layer.21.layernorm_after.bias', 'beit.encoder.layer.22.lambda_1', 'beit.encoder.layer.22.lambda_2', 'beit.encoder.layer.22.attention.attention.query.weight', 'beit.encoder.layer.22.attention.attention.query.bias', 'beit.encoder.layer.22.attention.attention.key.weight', 'beit.encoder.layer.22.attention.attention.value.weight', 'beit.encoder.layer.22.attention.attention.value.bias', 'beit.encoder.layer.22.attention.output.dense.weight', 'beit.encoder.layer.22.attention.output.dense.bias', 'beit.encoder.layer.22.intermediate.dense.weight', 'beit.encoder.layer.22.intermediate.dense.bias', 'beit.encoder.layer.22.output.dense.weight', 'beit.encoder.layer.22.output.dense.bias', 'beit.encoder.layer.22.layernorm_before.weight', 'beit.encoder.layer.22.layernorm_before.bias', 'beit.encoder.layer.22.layernorm_after.weight', 'beit.encoder.layer.22.layernorm_after.bias', 'beit.encoder.layer.23.lambda_1', 'beit.encoder.layer.23.lambda_2', 'beit.encoder.layer.23.attention.attention.query.weight', 'beit.encoder.layer.23.attention.attention.query.bias', 'beit.encoder.layer.23.attention.attention.key.weight', 'beit.encoder.layer.23.attention.attention.value.weight', 'beit.encoder.layer.23.attention.attention.value.bias', 'beit.encoder.layer.23.attention.output.dense.weight', 'beit.encoder.layer.23.attention.output.dense.bias', 'beit.encoder.layer.23.intermediate.dense.weight', 'beit.encoder.layer.23.intermediate.dense.bias', 'beit.encoder.layer.23.output.dense.weight', 'beit.encoder.layer.23.output.dense.bias', 'beit.encoder.layer.23.layernorm_before.weight', 'beit.encoder.layer.23.layernorm_before.bias', 'beit.encoder.layer.23.layernorm_after.weight', 'beit.encoder.layer.23.layernorm_after.bias', 'beit.pooler.layernorm.weight', 'beit.pooler.layernorm.bias', 'classifier.weight', 'classifier.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_4812\\2090117231.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_beit = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='115' max='115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [115/115 01:46, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>0.631534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.700516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.562800</td>\n",
       "      <td>0.300123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.304500</td>\n",
       "      <td>0.285776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.212900</td>\n",
       "      <td>0.257985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8782\n",
      "Precision (weighted): 0.8992\n",
      "Recall (weighted): 0.9386\n",
      "F1-Score (weighted): 0.9185\n",
      "Specificity: 0.7143\n",
      "AUC: 0.8264\n"
     ]
    }
   ],
   "source": [
    "T_and_T(breast_dataset_balanced_224, \"microsoft/beit-large-patch16-224-pt22k\", \"beit_breast_balanced_224\", 16, 0.1, True, True, beit_freezer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-large-patch16-224-pt22k and are newly initialized: ['beit.pooler.layernorm.bias', 'beit.pooler.layernorm.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['beit.encoder.layer.19.lambda_1', 'beit.encoder.layer.19.lambda_2', 'beit.encoder.layer.19.attention.attention.query.weight', 'beit.encoder.layer.19.attention.attention.query.bias', 'beit.encoder.layer.19.attention.attention.key.weight', 'beit.encoder.layer.19.attention.attention.value.weight', 'beit.encoder.layer.19.attention.attention.value.bias', 'beit.encoder.layer.19.attention.output.dense.weight', 'beit.encoder.layer.19.attention.output.dense.bias', 'beit.encoder.layer.19.intermediate.dense.weight', 'beit.encoder.layer.19.intermediate.dense.bias', 'beit.encoder.layer.19.output.dense.weight', 'beit.encoder.layer.19.output.dense.bias', 'beit.encoder.layer.19.layernorm_before.weight', 'beit.encoder.layer.19.layernorm_before.bias', 'beit.encoder.layer.19.layernorm_after.weight', 'beit.encoder.layer.19.layernorm_after.bias', 'beit.encoder.layer.20.lambda_1', 'beit.encoder.layer.20.lambda_2', 'beit.encoder.layer.20.attention.attention.query.weight', 'beit.encoder.layer.20.attention.attention.query.bias', 'beit.encoder.layer.20.attention.attention.key.weight', 'beit.encoder.layer.20.attention.attention.value.weight', 'beit.encoder.layer.20.attention.attention.value.bias', 'beit.encoder.layer.20.attention.output.dense.weight', 'beit.encoder.layer.20.attention.output.dense.bias', 'beit.encoder.layer.20.intermediate.dense.weight', 'beit.encoder.layer.20.intermediate.dense.bias', 'beit.encoder.layer.20.output.dense.weight', 'beit.encoder.layer.20.output.dense.bias', 'beit.encoder.layer.20.layernorm_before.weight', 'beit.encoder.layer.20.layernorm_before.bias', 'beit.encoder.layer.20.layernorm_after.weight', 'beit.encoder.layer.20.layernorm_after.bias', 'beit.encoder.layer.21.lambda_1', 'beit.encoder.layer.21.lambda_2', 'beit.encoder.layer.21.attention.attention.query.weight', 'beit.encoder.layer.21.attention.attention.query.bias', 'beit.encoder.layer.21.attention.attention.key.weight', 'beit.encoder.layer.21.attention.attention.value.weight', 'beit.encoder.layer.21.attention.attention.value.bias', 'beit.encoder.layer.21.attention.output.dense.weight', 'beit.encoder.layer.21.attention.output.dense.bias', 'beit.encoder.layer.21.intermediate.dense.weight', 'beit.encoder.layer.21.intermediate.dense.bias', 'beit.encoder.layer.21.output.dense.weight', 'beit.encoder.layer.21.output.dense.bias', 'beit.encoder.layer.21.layernorm_before.weight', 'beit.encoder.layer.21.layernorm_before.bias', 'beit.encoder.layer.21.layernorm_after.weight', 'beit.encoder.layer.21.layernorm_after.bias', 'beit.encoder.layer.22.lambda_1', 'beit.encoder.layer.22.lambda_2', 'beit.encoder.layer.22.attention.attention.query.weight', 'beit.encoder.layer.22.attention.attention.query.bias', 'beit.encoder.layer.22.attention.attention.key.weight', 'beit.encoder.layer.22.attention.attention.value.weight', 'beit.encoder.layer.22.attention.attention.value.bias', 'beit.encoder.layer.22.attention.output.dense.weight', 'beit.encoder.layer.22.attention.output.dense.bias', 'beit.encoder.layer.22.intermediate.dense.weight', 'beit.encoder.layer.22.intermediate.dense.bias', 'beit.encoder.layer.22.output.dense.weight', 'beit.encoder.layer.22.output.dense.bias', 'beit.encoder.layer.22.layernorm_before.weight', 'beit.encoder.layer.22.layernorm_before.bias', 'beit.encoder.layer.22.layernorm_after.weight', 'beit.encoder.layer.22.layernorm_after.bias', 'beit.encoder.layer.23.lambda_1', 'beit.encoder.layer.23.lambda_2', 'beit.encoder.layer.23.attention.attention.query.weight', 'beit.encoder.layer.23.attention.attention.query.bias', 'beit.encoder.layer.23.attention.attention.key.weight', 'beit.encoder.layer.23.attention.attention.value.weight', 'beit.encoder.layer.23.attention.attention.value.bias', 'beit.encoder.layer.23.attention.output.dense.weight', 'beit.encoder.layer.23.attention.output.dense.bias', 'beit.encoder.layer.23.intermediate.dense.weight', 'beit.encoder.layer.23.intermediate.dense.bias', 'beit.encoder.layer.23.output.dense.weight', 'beit.encoder.layer.23.output.dense.bias', 'beit.encoder.layer.23.layernorm_before.weight', 'beit.encoder.layer.23.layernorm_before.bias', 'beit.encoder.layer.23.layernorm_after.weight', 'beit.encoder.layer.23.layernorm_after.bias', 'beit.pooler.layernorm.weight', 'beit.pooler.layernorm.bias', 'classifier.weight', 'classifier.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_4812\\2090117231.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_beit = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1475' max='1475' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1475/1475 15:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.188500</td>\n",
       "      <td>0.259616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.086100</td>\n",
       "      <td>0.080435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.070400</td>\n",
       "      <td>0.112633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.089487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.085175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8878\n",
      "Precision (weighted): 0.8524\n",
      "Recall (weighted): 0.9923\n",
      "F1-Score (weighted): 0.9171\n",
      "Specificity: 0.7137\n",
      "AUC: 0.8530\n"
     ]
    }
   ],
   "source": [
    "T_and_T(pneumonia_dataset_28, \"microsoft/beit-large-patch16-224-pt22k\", \"beit_pneumonia_28\", 16, 0.1, True, True, beit_freezer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-large-patch16-224-pt22k and are newly initialized: ['beit.pooler.layernorm.bias', 'beit.pooler.layernorm.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['beit.encoder.layer.19.lambda_1', 'beit.encoder.layer.19.lambda_2', 'beit.encoder.layer.19.attention.attention.query.weight', 'beit.encoder.layer.19.attention.attention.query.bias', 'beit.encoder.layer.19.attention.attention.key.weight', 'beit.encoder.layer.19.attention.attention.value.weight', 'beit.encoder.layer.19.attention.attention.value.bias', 'beit.encoder.layer.19.attention.output.dense.weight', 'beit.encoder.layer.19.attention.output.dense.bias', 'beit.encoder.layer.19.intermediate.dense.weight', 'beit.encoder.layer.19.intermediate.dense.bias', 'beit.encoder.layer.19.output.dense.weight', 'beit.encoder.layer.19.output.dense.bias', 'beit.encoder.layer.19.layernorm_before.weight', 'beit.encoder.layer.19.layernorm_before.bias', 'beit.encoder.layer.19.layernorm_after.weight', 'beit.encoder.layer.19.layernorm_after.bias', 'beit.encoder.layer.20.lambda_1', 'beit.encoder.layer.20.lambda_2', 'beit.encoder.layer.20.attention.attention.query.weight', 'beit.encoder.layer.20.attention.attention.query.bias', 'beit.encoder.layer.20.attention.attention.key.weight', 'beit.encoder.layer.20.attention.attention.value.weight', 'beit.encoder.layer.20.attention.attention.value.bias', 'beit.encoder.layer.20.attention.output.dense.weight', 'beit.encoder.layer.20.attention.output.dense.bias', 'beit.encoder.layer.20.intermediate.dense.weight', 'beit.encoder.layer.20.intermediate.dense.bias', 'beit.encoder.layer.20.output.dense.weight', 'beit.encoder.layer.20.output.dense.bias', 'beit.encoder.layer.20.layernorm_before.weight', 'beit.encoder.layer.20.layernorm_before.bias', 'beit.encoder.layer.20.layernorm_after.weight', 'beit.encoder.layer.20.layernorm_after.bias', 'beit.encoder.layer.21.lambda_1', 'beit.encoder.layer.21.lambda_2', 'beit.encoder.layer.21.attention.attention.query.weight', 'beit.encoder.layer.21.attention.attention.query.bias', 'beit.encoder.layer.21.attention.attention.key.weight', 'beit.encoder.layer.21.attention.attention.value.weight', 'beit.encoder.layer.21.attention.attention.value.bias', 'beit.encoder.layer.21.attention.output.dense.weight', 'beit.encoder.layer.21.attention.output.dense.bias', 'beit.encoder.layer.21.intermediate.dense.weight', 'beit.encoder.layer.21.intermediate.dense.bias', 'beit.encoder.layer.21.output.dense.weight', 'beit.encoder.layer.21.output.dense.bias', 'beit.encoder.layer.21.layernorm_before.weight', 'beit.encoder.layer.21.layernorm_before.bias', 'beit.encoder.layer.21.layernorm_after.weight', 'beit.encoder.layer.21.layernorm_after.bias', 'beit.encoder.layer.22.lambda_1', 'beit.encoder.layer.22.lambda_2', 'beit.encoder.layer.22.attention.attention.query.weight', 'beit.encoder.layer.22.attention.attention.query.bias', 'beit.encoder.layer.22.attention.attention.key.weight', 'beit.encoder.layer.22.attention.attention.value.weight', 'beit.encoder.layer.22.attention.attention.value.bias', 'beit.encoder.layer.22.attention.output.dense.weight', 'beit.encoder.layer.22.attention.output.dense.bias', 'beit.encoder.layer.22.intermediate.dense.weight', 'beit.encoder.layer.22.intermediate.dense.bias', 'beit.encoder.layer.22.output.dense.weight', 'beit.encoder.layer.22.output.dense.bias', 'beit.encoder.layer.22.layernorm_before.weight', 'beit.encoder.layer.22.layernorm_before.bias', 'beit.encoder.layer.22.layernorm_after.weight', 'beit.encoder.layer.22.layernorm_after.bias', 'beit.encoder.layer.23.lambda_1', 'beit.encoder.layer.23.lambda_2', 'beit.encoder.layer.23.attention.attention.query.weight', 'beit.encoder.layer.23.attention.attention.query.bias', 'beit.encoder.layer.23.attention.attention.key.weight', 'beit.encoder.layer.23.attention.attention.value.weight', 'beit.encoder.layer.23.attention.attention.value.bias', 'beit.encoder.layer.23.attention.output.dense.weight', 'beit.encoder.layer.23.attention.output.dense.bias', 'beit.encoder.layer.23.intermediate.dense.weight', 'beit.encoder.layer.23.intermediate.dense.bias', 'beit.encoder.layer.23.output.dense.weight', 'beit.encoder.layer.23.output.dense.bias', 'beit.encoder.layer.23.layernorm_before.weight', 'beit.encoder.layer.23.layernorm_before.bias', 'beit.encoder.layer.23.layernorm_after.weight', 'beit.encoder.layer.23.layernorm_after.bias', 'beit.pooler.layernorm.weight', 'beit.pooler.layernorm.bias', 'classifier.weight', 'classifier.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_4812\\2090117231.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_beit = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='950' max='950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [950/950 10:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>0.168538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.118800</td>\n",
       "      <td>0.157667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.113200</td>\n",
       "      <td>0.089077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.122651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.042900</td>\n",
       "      <td>0.123828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9103\n",
      "Precision (weighted): 0.8884\n",
      "Recall (weighted): 0.9795\n",
      "F1-Score (weighted): 0.9317\n",
      "Specificity: 0.7949\n",
      "AUC: 0.8872\n"
     ]
    }
   ],
   "source": [
    "T_and_T(pneumonia_dataset_balanced_28, \"microsoft/beit-large-patch16-224-pt22k\", \"beit_pneumonia_balanced_28\", 16, 0.1, True, True, beit_freezer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-large-patch16-224-pt22k and are newly initialized: ['beit.pooler.layernorm.bias', 'beit.pooler.layernorm.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['beit.encoder.layer.19.lambda_1', 'beit.encoder.layer.19.lambda_2', 'beit.encoder.layer.19.attention.attention.query.weight', 'beit.encoder.layer.19.attention.attention.query.bias', 'beit.encoder.layer.19.attention.attention.key.weight', 'beit.encoder.layer.19.attention.attention.value.weight', 'beit.encoder.layer.19.attention.attention.value.bias', 'beit.encoder.layer.19.attention.output.dense.weight', 'beit.encoder.layer.19.attention.output.dense.bias', 'beit.encoder.layer.19.intermediate.dense.weight', 'beit.encoder.layer.19.intermediate.dense.bias', 'beit.encoder.layer.19.output.dense.weight', 'beit.encoder.layer.19.output.dense.bias', 'beit.encoder.layer.19.layernorm_before.weight', 'beit.encoder.layer.19.layernorm_before.bias', 'beit.encoder.layer.19.layernorm_after.weight', 'beit.encoder.layer.19.layernorm_after.bias', 'beit.encoder.layer.20.lambda_1', 'beit.encoder.layer.20.lambda_2', 'beit.encoder.layer.20.attention.attention.query.weight', 'beit.encoder.layer.20.attention.attention.query.bias', 'beit.encoder.layer.20.attention.attention.key.weight', 'beit.encoder.layer.20.attention.attention.value.weight', 'beit.encoder.layer.20.attention.attention.value.bias', 'beit.encoder.layer.20.attention.output.dense.weight', 'beit.encoder.layer.20.attention.output.dense.bias', 'beit.encoder.layer.20.intermediate.dense.weight', 'beit.encoder.layer.20.intermediate.dense.bias', 'beit.encoder.layer.20.output.dense.weight', 'beit.encoder.layer.20.output.dense.bias', 'beit.encoder.layer.20.layernorm_before.weight', 'beit.encoder.layer.20.layernorm_before.bias', 'beit.encoder.layer.20.layernorm_after.weight', 'beit.encoder.layer.20.layernorm_after.bias', 'beit.encoder.layer.21.lambda_1', 'beit.encoder.layer.21.lambda_2', 'beit.encoder.layer.21.attention.attention.query.weight', 'beit.encoder.layer.21.attention.attention.query.bias', 'beit.encoder.layer.21.attention.attention.key.weight', 'beit.encoder.layer.21.attention.attention.value.weight', 'beit.encoder.layer.21.attention.attention.value.bias', 'beit.encoder.layer.21.attention.output.dense.weight', 'beit.encoder.layer.21.attention.output.dense.bias', 'beit.encoder.layer.21.intermediate.dense.weight', 'beit.encoder.layer.21.intermediate.dense.bias', 'beit.encoder.layer.21.output.dense.weight', 'beit.encoder.layer.21.output.dense.bias', 'beit.encoder.layer.21.layernorm_before.weight', 'beit.encoder.layer.21.layernorm_before.bias', 'beit.encoder.layer.21.layernorm_after.weight', 'beit.encoder.layer.21.layernorm_after.bias', 'beit.encoder.layer.22.lambda_1', 'beit.encoder.layer.22.lambda_2', 'beit.encoder.layer.22.attention.attention.query.weight', 'beit.encoder.layer.22.attention.attention.query.bias', 'beit.encoder.layer.22.attention.attention.key.weight', 'beit.encoder.layer.22.attention.attention.value.weight', 'beit.encoder.layer.22.attention.attention.value.bias', 'beit.encoder.layer.22.attention.output.dense.weight', 'beit.encoder.layer.22.attention.output.dense.bias', 'beit.encoder.layer.22.intermediate.dense.weight', 'beit.encoder.layer.22.intermediate.dense.bias', 'beit.encoder.layer.22.output.dense.weight', 'beit.encoder.layer.22.output.dense.bias', 'beit.encoder.layer.22.layernorm_before.weight', 'beit.encoder.layer.22.layernorm_before.bias', 'beit.encoder.layer.22.layernorm_after.weight', 'beit.encoder.layer.22.layernorm_after.bias', 'beit.encoder.layer.23.lambda_1', 'beit.encoder.layer.23.lambda_2', 'beit.encoder.layer.23.attention.attention.query.weight', 'beit.encoder.layer.23.attention.attention.query.bias', 'beit.encoder.layer.23.attention.attention.key.weight', 'beit.encoder.layer.23.attention.attention.value.weight', 'beit.encoder.layer.23.attention.attention.value.bias', 'beit.encoder.layer.23.attention.output.dense.weight', 'beit.encoder.layer.23.attention.output.dense.bias', 'beit.encoder.layer.23.intermediate.dense.weight', 'beit.encoder.layer.23.intermediate.dense.bias', 'beit.encoder.layer.23.output.dense.weight', 'beit.encoder.layer.23.output.dense.bias', 'beit.encoder.layer.23.layernorm_before.weight', 'beit.encoder.layer.23.layernorm_before.bias', 'beit.encoder.layer.23.layernorm_after.weight', 'beit.encoder.layer.23.layernorm_after.bias', 'beit.pooler.layernorm.weight', 'beit.pooler.layernorm.bias', 'classifier.weight', 'classifier.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_4812\\2090117231.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_beit = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1475' max='1475' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1475/1475 15:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.084500</td>\n",
       "      <td>0.082364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.083100</td>\n",
       "      <td>0.086143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.021500</td>\n",
       "      <td>0.082897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.050427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.057570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8878\n",
      "Precision (weighted): 0.8493\n",
      "Recall (weighted): 0.9974\n",
      "F1-Score (weighted): 0.9175\n",
      "Specificity: 0.7051\n",
      "AUC: 0.8513\n"
     ]
    }
   ],
   "source": [
    "T_and_T(pneumonia_dataset_224, \"microsoft/beit-large-patch16-224-pt22k\", \"beit_pneumonia_224\", 16, 0.1, True, True, beit_freezer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-large-patch16-224-pt22k and are newly initialized: ['beit.pooler.layernorm.bias', 'beit.pooler.layernorm.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['beit.encoder.layer.19.lambda_1', 'beit.encoder.layer.19.lambda_2', 'beit.encoder.layer.19.attention.attention.query.weight', 'beit.encoder.layer.19.attention.attention.query.bias', 'beit.encoder.layer.19.attention.attention.key.weight', 'beit.encoder.layer.19.attention.attention.value.weight', 'beit.encoder.layer.19.attention.attention.value.bias', 'beit.encoder.layer.19.attention.output.dense.weight', 'beit.encoder.layer.19.attention.output.dense.bias', 'beit.encoder.layer.19.intermediate.dense.weight', 'beit.encoder.layer.19.intermediate.dense.bias', 'beit.encoder.layer.19.output.dense.weight', 'beit.encoder.layer.19.output.dense.bias', 'beit.encoder.layer.19.layernorm_before.weight', 'beit.encoder.layer.19.layernorm_before.bias', 'beit.encoder.layer.19.layernorm_after.weight', 'beit.encoder.layer.19.layernorm_after.bias', 'beit.encoder.layer.20.lambda_1', 'beit.encoder.layer.20.lambda_2', 'beit.encoder.layer.20.attention.attention.query.weight', 'beit.encoder.layer.20.attention.attention.query.bias', 'beit.encoder.layer.20.attention.attention.key.weight', 'beit.encoder.layer.20.attention.attention.value.weight', 'beit.encoder.layer.20.attention.attention.value.bias', 'beit.encoder.layer.20.attention.output.dense.weight', 'beit.encoder.layer.20.attention.output.dense.bias', 'beit.encoder.layer.20.intermediate.dense.weight', 'beit.encoder.layer.20.intermediate.dense.bias', 'beit.encoder.layer.20.output.dense.weight', 'beit.encoder.layer.20.output.dense.bias', 'beit.encoder.layer.20.layernorm_before.weight', 'beit.encoder.layer.20.layernorm_before.bias', 'beit.encoder.layer.20.layernorm_after.weight', 'beit.encoder.layer.20.layernorm_after.bias', 'beit.encoder.layer.21.lambda_1', 'beit.encoder.layer.21.lambda_2', 'beit.encoder.layer.21.attention.attention.query.weight', 'beit.encoder.layer.21.attention.attention.query.bias', 'beit.encoder.layer.21.attention.attention.key.weight', 'beit.encoder.layer.21.attention.attention.value.weight', 'beit.encoder.layer.21.attention.attention.value.bias', 'beit.encoder.layer.21.attention.output.dense.weight', 'beit.encoder.layer.21.attention.output.dense.bias', 'beit.encoder.layer.21.intermediate.dense.weight', 'beit.encoder.layer.21.intermediate.dense.bias', 'beit.encoder.layer.21.output.dense.weight', 'beit.encoder.layer.21.output.dense.bias', 'beit.encoder.layer.21.layernorm_before.weight', 'beit.encoder.layer.21.layernorm_before.bias', 'beit.encoder.layer.21.layernorm_after.weight', 'beit.encoder.layer.21.layernorm_after.bias', 'beit.encoder.layer.22.lambda_1', 'beit.encoder.layer.22.lambda_2', 'beit.encoder.layer.22.attention.attention.query.weight', 'beit.encoder.layer.22.attention.attention.query.bias', 'beit.encoder.layer.22.attention.attention.key.weight', 'beit.encoder.layer.22.attention.attention.value.weight', 'beit.encoder.layer.22.attention.attention.value.bias', 'beit.encoder.layer.22.attention.output.dense.weight', 'beit.encoder.layer.22.attention.output.dense.bias', 'beit.encoder.layer.22.intermediate.dense.weight', 'beit.encoder.layer.22.intermediate.dense.bias', 'beit.encoder.layer.22.output.dense.weight', 'beit.encoder.layer.22.output.dense.bias', 'beit.encoder.layer.22.layernorm_before.weight', 'beit.encoder.layer.22.layernorm_before.bias', 'beit.encoder.layer.22.layernorm_after.weight', 'beit.encoder.layer.22.layernorm_after.bias', 'beit.encoder.layer.23.lambda_1', 'beit.encoder.layer.23.lambda_2', 'beit.encoder.layer.23.attention.attention.query.weight', 'beit.encoder.layer.23.attention.attention.query.bias', 'beit.encoder.layer.23.attention.attention.key.weight', 'beit.encoder.layer.23.attention.attention.value.weight', 'beit.encoder.layer.23.attention.attention.value.bias', 'beit.encoder.layer.23.attention.output.dense.weight', 'beit.encoder.layer.23.attention.output.dense.bias', 'beit.encoder.layer.23.intermediate.dense.weight', 'beit.encoder.layer.23.intermediate.dense.bias', 'beit.encoder.layer.23.output.dense.weight', 'beit.encoder.layer.23.output.dense.bias', 'beit.encoder.layer.23.layernorm_before.weight', 'beit.encoder.layer.23.layernorm_before.bias', 'beit.encoder.layer.23.layernorm_after.weight', 'beit.encoder.layer.23.layernorm_after.bias', 'beit.pooler.layernorm.weight', 'beit.pooler.layernorm.bias', 'classifier.weight', 'classifier.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_4812\\2090117231.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_beit = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='950' max='950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [950/950 10:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.074700</td>\n",
       "      <td>0.059993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.051000</td>\n",
       "      <td>0.136266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.067600</td>\n",
       "      <td>0.093167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.067371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.068900</td>\n",
       "      <td>0.070074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9151\n",
      "Precision (weighted): 0.8821\n",
      "Recall (weighted): 0.9974\n",
      "F1-Score (weighted): 0.9362\n",
      "Specificity: 0.7778\n",
      "AUC: 0.8876\n"
     ]
    }
   ],
   "source": [
    "T_and_T(pneumonia_dataset_balanced_224, \"microsoft/beit-large-patch16-224-pt22k\", \"beit_pneumonia_balanced_224\", 16, 0.1, True, True, beit_freezer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-50\", num_labels=2, ignore_mismatched_sizes=True)\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([2, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['resnet.encoder.stages.3.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.bias', 'classifier.1.weight', 'classifier.1.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_4812\\2090117231.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_beit = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [175/175 00:14, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.529100</td>\n",
       "      <td>0.620440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.596400</td>\n",
       "      <td>0.555208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.515500</td>\n",
       "      <td>0.534902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.468900</td>\n",
       "      <td>0.522058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.494700</td>\n",
       "      <td>0.516822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7308\n",
      "Precision (weighted): 0.7308\n",
      "Recall (weighted): 1.0000\n",
      "F1-Score (weighted): 0.8444\n",
      "Specificity: 0.0000\n",
      "AUC: 0.5000\n"
     ]
    }
   ],
   "source": [
    "T_and_T(breast_dataset_28, \"microsoft/resnet-50\", \"resnet_breast_28\", 16, 0, True, True, resnet_freezer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([2, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['resnet.encoder.stages.3.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.bias', 'classifier.1.weight', 'classifier.1.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_4812\\2090117231.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_beit = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='115' max='115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [115/115 00:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.675259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.653000</td>\n",
       "      <td>0.658845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.627600</td>\n",
       "      <td>0.575005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.621700</td>\n",
       "      <td>0.569778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.624300</td>\n",
       "      <td>0.552282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7308\n",
      "Precision (weighted): 0.7308\n",
      "Recall (weighted): 1.0000\n",
      "F1-Score (weighted): 0.8444\n",
      "Specificity: 0.0000\n",
      "AUC: 0.5000\n"
     ]
    }
   ],
   "source": [
    "T_and_T(breast_dataset_balanced_28, \"microsoft/resnet-50\", \"resnet_breast_balanced_28\", 16, 0, True, True, resnet_freezer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([2, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['resnet.encoder.stages.3.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.bias', 'classifier.1.weight', 'classifier.1.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_4812\\2090117231.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_beit = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [175/175 00:18, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.522800</td>\n",
       "      <td>0.616823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.597100</td>\n",
       "      <td>0.558393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.514900</td>\n",
       "      <td>0.543286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.456100</td>\n",
       "      <td>0.532300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.497200</td>\n",
       "      <td>0.530578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7308\n",
      "Precision (weighted): 0.7308\n",
      "Recall (weighted): 1.0000\n",
      "F1-Score (weighted): 0.8444\n",
      "Specificity: 0.0000\n",
      "AUC: 0.5000\n"
     ]
    }
   ],
   "source": [
    "T_and_T(breast_dataset_224, \"microsoft/resnet-50\", \"resnet_breast_224\", 16, 0, True, True, resnet_freezer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([2, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['resnet.encoder.stages.3.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.bias', 'classifier.1.weight', 'classifier.1.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_4812\\2090117231.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_beit = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='115' max='115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [115/115 00:13, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.673500</td>\n",
       "      <td>0.652453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.644100</td>\n",
       "      <td>0.629852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.625800</td>\n",
       "      <td>0.584987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.603200</td>\n",
       "      <td>0.572080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.610300</td>\n",
       "      <td>0.559907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7500\n",
      "Precision (weighted): 0.7451\n",
      "Recall (weighted): 1.0000\n",
      "F1-Score (weighted): 0.8539\n",
      "Specificity: 0.0714\n",
      "AUC: 0.5357\n"
     ]
    }
   ],
   "source": [
    "T_and_T(breast_dataset_balanced_224, \"microsoft/resnet-50\", \"resnet_breast_balanced_224\", 16, 0, True, True, resnet_freezer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([2, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['resnet.encoder.stages.3.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.bias', 'classifier.1.weight', 'classifier.1.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_4812\\2090117231.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_beit = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1475' max='1475' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1475/1475 01:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.226300</td>\n",
       "      <td>0.217299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.214700</td>\n",
       "      <td>0.186368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.182300</td>\n",
       "      <td>0.168383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.165800</td>\n",
       "      <td>0.168197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.139800</td>\n",
       "      <td>0.163736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8093\n",
      "Precision (weighted): 0.7794\n",
      "Recall (weighted): 0.9692\n",
      "F1-Score (weighted): 0.8640\n",
      "Specificity: 0.5427\n",
      "AUC: 0.7560\n"
     ]
    }
   ],
   "source": [
    "T_and_T(pneumonia_dataset_28, \"microsoft/resnet-50\", \"resnet_pneumonia_28\", 16, 0, True, True, resnet_freezer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([2, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['resnet.encoder.stages.3.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.bias', 'classifier.1.weight', 'classifier.1.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_4812\\2090117231.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_beit = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='950' max='950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [950/950 01:17, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.323900</td>\n",
       "      <td>0.252124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.179100</td>\n",
       "      <td>0.217092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.219700</td>\n",
       "      <td>0.195487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.208000</td>\n",
       "      <td>0.216723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.191600</td>\n",
       "      <td>0.200185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8478\n",
      "Precision (weighted): 0.8375\n",
      "Recall (weighted): 0.9385\n",
      "F1-Score (weighted): 0.8851\n",
      "Specificity: 0.6966\n",
      "AUC: 0.8175\n"
     ]
    }
   ],
   "source": [
    "T_and_T(pneumonia_dataset_balanced_28, \"microsoft/resnet-50\", \"resnet_pneumonia_balanced_28\", 16, 0, True, True, resnet_freezer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([2, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['resnet.encoder.stages.3.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.bias', 'classifier.1.weight', 'classifier.1.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_4812\\2090117231.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_beit = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1475' max='1475' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1475/1475 02:19, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.178764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.142000</td>\n",
       "      <td>0.152917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.141200</td>\n",
       "      <td>0.136318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.123528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.124100</td>\n",
       "      <td>0.124081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8606\n",
      "Precision (weighted): 0.8230\n",
      "Recall (weighted): 0.9897\n",
      "F1-Score (weighted): 0.8987\n",
      "Specificity: 0.6453\n",
      "AUC: 0.8175\n"
     ]
    }
   ],
   "source": [
    "T_and_T(pneumonia_dataset_224, \"microsoft/resnet-50\", \"resnet_pneumonia_224\", 16, 0, True, True, resnet_freezer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([2, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['resnet.encoder.stages.3.layers.2.layer.0.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.0.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.1.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.1.normalization.bias', 'resnet.encoder.stages.3.layers.2.layer.2.convolution.weight', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.weight', 'resnet.encoder.stages.3.layers.2.layer.2.normalization.bias', 'classifier.1.weight', 'classifier.1.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baiet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baiet\\AppData\\Local\\Temp\\ipykernel_4812\\2090117231.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_beit = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='950' max='950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [950/950 01:39, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.248100</td>\n",
       "      <td>0.177316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.162700</td>\n",
       "      <td>0.174311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.179400</td>\n",
       "      <td>0.145925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.151000</td>\n",
       "      <td>0.153527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.137600</td>\n",
       "      <td>0.164797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8942\n",
      "Precision (weighted): 0.8750\n",
      "Recall (weighted): 0.9692\n",
      "F1-Score (weighted): 0.9197\n",
      "Specificity: 0.7692\n",
      "AUC: 0.8692\n"
     ]
    }
   ],
   "source": [
    "T_and_T(pneumonia_dataset_balanced_224, \"microsoft/resnet-50\", \"resnet_pneumonia_balanced_224\", 16, 0.1, True, True, resnet_freezer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
